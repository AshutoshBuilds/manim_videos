{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992b7670-c90e-47f7-b8fa-84cb59f5be60",
   "metadata": {},
   "source": [
    "## Hackin 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dd01d-360e-4134-8bdf-9d9c16353969",
   "metadata": {},
   "source": [
    "- Ok ok ok ok ok ok\n",
    "- I dont want to overcomplicate things here\n",
    "- I think this visual could be dope, and all I need at this phase is some proof of concept that I can see some meaningful difference betwen types of examples\n",
    "- I dont' have to even train, just compute grads and visualize I think, and see if my my quick/rough max-pooley appraoch can reveal some pattters here\n",
    "- Claude had some good ideas for types of eamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0605d7-4c3e-4741-ad83-c346e337fa21",
   "metadata": {},
   "source": [
    "- Code completation vs creative writing\n",
    "- Technical explanations vs emotional/narrative text\n",
    "- math problems\n",
    "- factual recall\n",
    "- Short vs long\n",
    "- repeated combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f37392-f145-4742-8f29-1f0fd5860c5d",
   "metadata": {},
   "source": [
    "Ok let me make a few baby datasets -> maybe like 5-7 examples each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06d0e5-0e93-42b9-92dd-b8a941df848f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1f3d5b-cecd-42a3-9d15-b7c5f91536a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers matplotlib tqdm huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac30ba9-2e99-4f28-a6a1-02a16d1f3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50828e1-fa2d-44ee-8529-7529f5b52158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c755533-e25e-4a43-9503-804efcff5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baby_dataset_so_cute=[\n",
    "#     \"2 + 2 = \",\n",
    "#     \"4 + 3 = \",\n",
    "#     \"2 + 9 = \",\n",
    "#     \"11 - 10 = \",\n",
    "#     \"3 - 6 = \", #Gets this one wrong\n",
    "#     \"3 * 6 = \",\n",
    "#     \"3 * 2 = \",\n",
    "#     \"8 / 2 = \",\n",
    "#     \"128 / 32 = \",\n",
    "#     \"The capital of France is\",\n",
    "    \n",
    "# ]\n",
    "\n",
    "baby_dataset_so_cute=[\n",
    "    #jibberish\n",
    "    \"asdflkjasdf\",\n",
    "    \"189sdfsf8sdf8sns\",\n",
    "    \"asdfasdfuowofof\",\n",
    "    \n",
    "    # Arithmetic (10 examples)\n",
    "    \"2 + 2 = \",\n",
    "    \"4 + 3 = \",\n",
    "    \"2 + 9 = \",\n",
    "    \"11 - 10 = \",\n",
    "    \"3 - 6 = \",  # Gets this one wrong\n",
    "    \"3 * 6 = \",\n",
    "    \"3 * 2 = \",\n",
    "    \"8 / 2 = \",\n",
    "    \"128 / 32 = \",\n",
    "    \"9 * 7 = \",\n",
    "    \n",
    "    # Geography facts (8 examples)\n",
    "    \"The capital of France is\",\n",
    "    \"The capital of Japan is\",\n",
    "    \"The capital of Brazil is\",\n",
    "    \"Mount Everest is located in\",\n",
    "    \"The Amazon River flows through\",\n",
    "    \"The largest continent is\",\n",
    "    \"The Pacific Ocean borders\",\n",
    "    \"The currency of China is\",\n",
    "    \n",
    "    # Sports facts (8 examples)\n",
    "    \"The Lakers play in\",\n",
    "    \"The World Cup is held every\",\n",
    "    \"Serena Williams plays\",\n",
    "    \"The Super Bowl happens in\",\n",
    "    \"A basketball team has\",\n",
    "    \"The Olympics occur every\",\n",
    "    \"Tiger Woods is famous for\",\n",
    "    \"Cristiano Ronaldo plays\",\n",
    "    \n",
    "    # Code (10 examples)\n",
    "    \"def add_numbers(a, b):\",\n",
    "    \"import numpy as\",\n",
    "    \"for i in range(\",\n",
    "    \"if x > 0:\",\n",
    "    \"print('Hello\",\n",
    "    \"class Dog:\",\n",
    "    \"return x +\",\n",
    "    \"try:\",\n",
    "    \"from datetime import\",\n",
    "    \"x = [1, 2, 3,\",\n",
    "    \n",
    "    # Creative writing (10 examples)\n",
    "    \"Once upon a time, in a magical forest\",\n",
    "    \"The old wizard looked up at the stars and\",\n",
    "    \"She opened the mysterious door and found\",\n",
    "    \"The dragon's eyes glowed softly as\",\n",
    "    \"In the bustling marketplace, children\",\n",
    "    \"As the sun set behind the mountains,\",\n",
    "    \"The little robot whirred to life and\",\n",
    "    \"Deep in the ocean, a mermaid\",\n",
    "    \"The spaceship landed with a gentle\",\n",
    "    \"Through the mist came the sound of\",\n",
    "\n",
    "    # Logical reasoning/word relationships (8 examples)\n",
    "    \"If all cats are animals, and Fluffy is a cat, then Fluffy is\",\n",
    "    \"Apple is to fruit as carrot is to\",\n",
    "    \"Hot is the opposite of\",\n",
    "    \"Bird is to fly as fish is to\",\n",
    "    \"Red, blue, green, yellow,\",  # sequence completion\n",
    "    \"Monday, Tuesday, Wednesday,\",\n",
    "    \"January comes before\",\n",
    "    \"Bigger than an elephant, smaller than a\",\n",
    "\n",
    "    #Indirect Object Identification\n",
    "    \"When John and Mary went to the shops, John gave the bag to Mary\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to James\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to Sid\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to Amy\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to John\",\n",
    "    \"When Tom and James went to the park, James gave the ball to Tom\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to Dan\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to Martin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3569721-e53a-46db-97f4-fce832348bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74b97e6-430b-4871-993a-5ca818bf3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device) #Numerical stability? Seems like this work? Saves a lot of memory!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3714ad66-072a-44d2-9f3a-d66851b7666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/workspace/may_12_8'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "kernel_size=64\n",
    "stride=64\n",
    "\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=kernel_size, stride=stride)\n",
    "max_pool=torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "tensor_names=['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', \n",
    "            'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985fc93a-9f65-4d27-8ff2-7b86394bfcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_params = {name: p for name, p in model.named_parameters() if p.requires_grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be838eca-ae5e-40f4-9903-ccbacdc9c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min=0\n",
    "global_max=0\n",
    "for k,v in filtered_params.items(): #Hmm i guess I'm doing the global weight average not the global grad average - do we care right now?\n",
    "    if v.max().item()>global_max:\n",
    "        global_max=v.max().item()\n",
    "    if v.min().item()<global_min:\n",
    "        global_min=v.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120a113f-a310-4630-8929-57b6bcbbd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fef9a-8910-472b-b05a-d8c772e0f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/57 [00:05<04:49,  5.18s/it]/tmp/ipykernel_4244/921253245.py:15: UserWarning: Ignoring specified arguments in this call because figure with num: 0 already exists\n",
      "  fig=plt.figure(0,(16,9), facecolor='k')\n",
      " 44%|████▍     | 25/57 [02:12<02:43,  5.12s/it]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(baby_dataset_so_cute))):\n",
    "    inputs = tokenizer(baby_dataset_so_cute[i], return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss #Ok not just paris loss here -> not sure how much I'm worried about that\n",
    "    loss.backward()\n",
    "    \n",
    "    # print(loss.item())\n",
    "    # optimizer.step() ##Don't actually train!\n",
    "\n",
    "    plt.clf()\n",
    "    fig=plt.figure(0,(16,9), facecolor='k')\n",
    "    for layer_num in range(16):\n",
    "        for tensor_index in range(len(tensor_names)):\n",
    "            tensor_name=tensor_names[tensor_index]\n",
    "            # w=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].detach().cpu()\n",
    "            g=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].grad.detach().cpu()\n",
    "            \n",
    "            # w_pooled=max_pool(w.unsqueeze(0))\n",
    "            g_pooled=max_pool(g.unsqueeze(0))\n",
    "            # g_pooled=avg_pool(g.unsqueeze(0))\n",
    "    \n",
    "            fig.add_subplot(7, 16, tensor_index*16+layer_num+1)    \n",
    "            plt.imshow(g_pooled[0], vmin=-0.12, vmax=0.12)\n",
    "            plt.axis('off')\n",
    "\n",
    "    # plt.axis('on')\n",
    "    fig.suptitle(baby_dataset_so_cute[i], color='w')\n",
    "    plt.savefig(save_dir + '/' + 'grads_max_pooled_global_norm_' + str(i).zfill(3) +'.png', dpi=150, facecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ec2bc-e7a2-4f4e-bc21-248c1110f7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dc3dc-e6ce-4637-9a9d-a9576b4dcc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec47b0-cf05-4f33-bbae-b301374726df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f2820-024b-472a-9f47-37903eb6b336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2faa354-1381-455b-8bd2-f7c77e3e1c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31967023-e2a8-41a1-b96a-97ec49e32353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ec7289-ca35-4d99-8649-6d78f3cec2c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d203244-e62b-4310-b9e6-14306ee501db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The capital of France is\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "sI=np.argsort(my_probs[0,-1, :].detach().cpu().float().numpy())[::-1]\n",
    "for i in sI[:10]:\n",
    "    print(i, round(my_probs[0, -1, i].item(),5), tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ee419-d454-47e0-bdba-ccc2e90907bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c05a72-5595-4513-8504-9e737425c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"The capital of France is Berlin\"\n",
    "# text = \"Born in Chicago, Illinois, United States and raised in a town near the city's South Side, Prevost became a friar of the Order of Saint Augustine in 1977 and was ordained priest in 1982. His service has included extensive missionary work in Peru from 1985 to 1986 and from 1988 to 1998, where he variously served as a parish pastor, diocesan official, seminary teacher, and administrator. Elected prior general of the Order of Saint Augustine from 2001 to 2013, he later returned to Peru as Bishop of Chiclayo from 2015 to 2023. In 2023, Pope Francis appointed him prefect of the Dicastery for Bishops and president of the Pontifical Commission for Latin America, and made him a cardinal the same year.\"\n",
    "text = \"2 + 2 = \"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "y_one_hot=F.one_hot(input_ids, num_classes=model.config.vocab_size)\n",
    "correct_next_token_probs = (my_probs[:,:-1]*y_one_hot[:,1:]).sum(-1) \n",
    "my_loss=-torch.log(correct_next_token_probs).mean()\n",
    "print(my_loss.item(), outputs.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857421cb-42c2-4293-b66f-c1db8e277dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "sI=np.argsort(my_probs[0,-1, :].detach().cpu().float().numpy())[::-1]\n",
    "for i in sI[:10]:\n",
    "    print(i, round(my_probs[0, -1, i].item(),5), tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afb58b-80e3-4301-87e3-d5e03a11e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/workspace/may_12_6'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "kernel_size=64\n",
    "stride=64\n",
    "\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=kernel_size, stride=stride)\n",
    "max_pool=torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "tensor_names=['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', \n",
    "            'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31038b73-830e-4192-8503-1a9d46a6fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_params = {name: p for name, p in model.named_parameters() if p.requires_grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8a56c-9ccf-40f1-a205-e00b2edd7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min=0\n",
    "global_max=0\n",
    "for k,v in filtered_params.items(): #Hmm i guess I'm doing the global weight average not the global grad average - do we care right now?\n",
    "    if v.max().item()>global_max:\n",
    "        global_max=v.max().item()\n",
    "    if v.min().item()<global_min:\n",
    "        global_min=v.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72518a6f-37a8-4163-b481-5066beafb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b16f0-35ea-4c2a-abba-c862ed45c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3779a79-8e80-4c97-b1c1-e5420e150f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(input_ids.to(device), labels=input_ids.to(device))\n",
    "out.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f1c88-a5f3-45bf-a9c6-1379318121f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(0,(16,9), facecolor='k')\n",
    "for layer_num in range(16):\n",
    "    for tensor_index in range(len(tensor_names)):\n",
    "        tensor_name=tensor_names[tensor_index]\n",
    "        # w=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].detach().cpu()\n",
    "        g=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].grad.detach().cpu()\n",
    "        \n",
    "        # w_pooled=max_pool(w.unsqueeze(0))\n",
    "        g_pooled=max_pool(g.unsqueeze(0))\n",
    "        # g_pooled=avg_pool(g.unsqueeze(0))\n",
    "\n",
    "        fig.add_subplot(7, 16, tensor_index*16+layer_num+1)    \n",
    "        plt.imshow(g_pooled[0], vmin=-0.025, vmax=0.025)\n",
    "        plt.axis('off')\n",
    "# plt.savefig(save_dir + '/' + 'grads_max_pooled_global_norm_0001' + '.png', dpi=150, facecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d655485-fd0e-41b6-a097-cd0df4d7e198",
   "metadata": {},
   "source": [
    "- I guess we also dont' have to really even train necessarily right? Like we could just viz gradients for various training examples? I'll want ot train at some point -> but I don't have to to gets some idea of what's going on here I think is the idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63e1a2-7430-4639-a8fe-536b613ed201",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac4a51-3d34-457d-889f-1985daa7a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss #Ok not just paris loss here -> not sure how much I'm worried about that\n",
    "    loss.backward()\n",
    "    print(loss.item())\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Add gradient clipping\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    plt.clf()\n",
    "    fig=plt.figure(0,(16,9), facecolor='k')\n",
    "    for layer_num in range(16):\n",
    "        for tensor_index in range(len(tensor_names)):\n",
    "            tensor_name=tensor_names[tensor_index]\n",
    "            # w=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].detach().cpu()\n",
    "            g=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].grad.detach().cpu()\n",
    "            \n",
    "            # w_pooled=max_pool(w.unsqueeze(0))\n",
    "            g_pooled=max_pool(g.unsqueeze(0))\n",
    "            # g_pooled=avg_pool(g.unsqueeze(0))\n",
    "    \n",
    "            fig.add_subplot(7, 16, tensor_index*16+layer_num+1)    \n",
    "            plt.imshow(g_pooled[0], vmin=-0.025, vmax=0.025)\n",
    "            plt.axis('off')\n",
    "    plt.savefig(save_dir + '/' + 'grads_max_pooled_global_norm_' + str(i).zfill(3) +'.png', dpi=150, facecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00674b-252a-49db-9c47-8dd76d3e647d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cc5bc-a6a0-4000-abad-4eb29db39121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d90b1f-cfb4-46c8-b279-0d446f0b2da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49906ac4-efe1-4a30-90b2-97aa79d4d6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdb633-59ff-4122-bb01-083598326d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14732a48-9b6d-476f-a5c8-21a8a120e672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf6c3c-765d-41fc-bcff-f435cdf82c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(input_ids.to(device), labels=input_ids.to(device))\n",
    "out.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c312ef-9b49-427e-8ee4-cce65de047df",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_params = {name: p for name, p in model.named_parameters() if p.requires_grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce9f0c-bfb4-4ef2-be77-b29a3abf500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705cab5-4a28-4316-acbf-4728049f0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss #Ok not just paris loss here -> not sure how much I'm worried about that\n",
    "    loss.backward()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # Add gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2e8ca-6c1a-423c-aae2-570d194cbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "sI=np.argsort(my_probs[0,5, :].detach().cpu().float().numpy())[::-1]\n",
    "for i in sI[:10]:\n",
    "    print(i, round(my_probs[0, 5, i].item(),5), tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09e558-fc01-4d99-bfbd-86eea1b6c553",
   "metadata": {},
   "source": [
    "- Ok yeah so my poor little 3090 just can't do it -> that's a bummer.\n",
    "- For some reason I do feel like I want to stick with Llama 1b -> os let's stick with llama 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1bfa58-a032-46ac-b4c8-5b0a136443e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3dd5c-0402-4b8b-a549-dfdb72fcb80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d817388-cd65-4d82-8f6f-3a233a6924d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2effa65-e4ab-4883-a823-96a499d55e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f16a5-51bb-4f96-accd-574c37219138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1cd44-892a-41b4-a228-1defefa0af5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d171d5a-9e8b-4e4d-935b-321856f28a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90b532-d3b1-4af2-ac91-d5e7e5630324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bec50d-9d24-48ca-8eaa-46365d6d6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='may_12_3'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "kernel_size=64\n",
    "stride=64\n",
    "\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=kernel_size, stride=stride)\n",
    "max_pool=torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "tensor_names=['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', \n",
    "            'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02d1e6-465a-4bf0-a604-b63c73d04d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(0,(16,9), facecolor='k')\n",
    "for layer_num in range(16):\n",
    "    for tensor_index in range(len(tensor_names)):\n",
    "        tensor_name=tensor_names[tensor_index]\n",
    "        # w=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].detach().cpu()\n",
    "        g=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].grad.detach().cpu()\n",
    "        \n",
    "        # w_pooled=max_pool(w.unsqueeze(0))\n",
    "        g_pooled=max_pool(g.unsqueeze(0))\n",
    "        # g_pooled=avg_pool(g.unsqueeze(0))\n",
    "\n",
    "        fig.add_subplot(7, 16, tensor_index*16+layer_num+1)    \n",
    "        plt.imshow(g_pooled[0], vmin=global_min*0.0001, vmax=global_max*0.0001)\n",
    "        plt.axis('off')\n",
    "plt.savefig(save_dir + '/' + 'grads_max_pooled_global_norm_0001' + '.png', dpi=150, facecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a6096-a679-4085-a8dc-48233f57989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min=0\n",
    "global_max=0\n",
    "for k,v in filtered_params.items(): #Hmm i guess I'm doing the global weight average not the global grad average - do we care right now?\n",
    "    if v.max().item()>global_max:\n",
    "        global_max=v.max().item()\n",
    "    if v.min().item()<global_min:\n",
    "        global_min=v.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d7407-7c1d-47a6-8f36-a9ed5094a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(0,(16,9), facecolor='k')\n",
    "for layer_num in range(16):\n",
    "    for tensor_index in range(len(tensor_names)):\n",
    "        tensor_name=tensor_names[tensor_index]\n",
    "        # w=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].detach().cpu()\n",
    "        g=filtered_params['model.layers.'+str(layer_num)+'.'+tensor_name].grad.detach().cpu()\n",
    "        \n",
    "        # w_pooled=max_pool(w.unsqueeze(0))\n",
    "        g_pooled=max_pool(g.unsqueeze(0))\n",
    "        # g_pooled=avg_pool(g.unsqueeze(0))\n",
    "\n",
    "        fig.add_subplot(7, 16, tensor_index*16+layer_num+1)    \n",
    "        plt.imshow(g_pooled[0], vmin=global_min*0.0001, vmax=global_max*0.0001)\n",
    "        plt.axis('off')\n",
    "plt.savefig(save_dir + '/' + 'grads_max_pooled_global_norm_0001' + '.png', dpi=150, facecolor='k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
