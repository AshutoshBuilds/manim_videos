{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56d1131-3818-4959-993e-8a39597af93e",
   "metadata": {},
   "source": [
    "## P47 Wormhole \n",
    "- Cleaned up version w/ original param replacement bug fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a72b6a-70cd-4d5c-b04c-6325114699aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers matplotlib tqdm huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918cc7c3-74f2-438c-ad8a-2cfb26f1e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85155a3b-c331-41c0-9402-8c3d05354ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1897e993-77db-4f3f-982f-b66740971ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "#Pretrained\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2b3cf-ae54-46b5-977f-3ae70981f487",
   "metadata": {},
   "source": [
    "## Configuration for this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88763c12-4b95-423e-ad59-6d0004ea456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='/workspace/apr_26_2'\n",
    "num_points=32 \n",
    "n_steps=128 \n",
    "lr=1e-7\n",
    "delayed_viz_start=64 #e.g. set to 10 if i only want to start renderding after the 10th optimiation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a3663-f0de-44f4-9c85-380c2a574107",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f4c1c5-1423-4880-969a-81a5c88d823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_directions(params, seed=None):\n",
    "    \"\"\"\n",
    "    Generate random direction vectors for each parameter tensor.\n",
    "    \n",
    "    Args:\n",
    "        params: List of (name, parameter) tuples from model.named_parameters()\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        direction: OrderedDict mapping parameter names to random direction tensors\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    direction = OrderedDict()\n",
    "    for name, param in params:\n",
    "        if param.requires_grad:\n",
    "            direction[name] = torch.randn_like(param.data)\n",
    "    \n",
    "    return direction\n",
    "\n",
    "def normalize_direction(direction, params):\n",
    "    \"\"\"\n",
    "    Normalize the direction tensors to match the norm of each parameter tensor.\n",
    "    \n",
    "    Args:\n",
    "        direction: OrderedDict mapping parameter names to direction tensors\n",
    "        params: List of (name, parameter) tuples from model.named_parameters()\n",
    "        \n",
    "    Returns:\n",
    "        normalized_direction: OrderedDict with normalized direction tensors\n",
    "    \"\"\"\n",
    "    param_dict = OrderedDict(params)\n",
    "    normalized_direction = OrderedDict()\n",
    "    \n",
    "    for name, dir_tensor in direction.items():\n",
    "        param_norm = torch.norm(param_dict[name].data)\n",
    "        dir_norm = torch.norm(dir_tensor)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if dir_norm > 0:\n",
    "            normalized_direction[name] = dir_tensor * (param_norm / dir_norm)\n",
    "        else:\n",
    "            normalized_direction[name] = dir_tensor\n",
    "    \n",
    "    return normalized_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4b292-7861-4942-91b2-482ea1ac1a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c197f99d-d206-470a-baf9-78d6e87f6cd4",
   "metadata": {},
   "source": [
    "### Setup example and run some Computation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d857cc4e-504b-43fa-a474-f7d3bc6bef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The capital of France is Paris\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48a7703-93f2-427a-92dd-4650c24ed8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3751845359802246 3.3751840591430664\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "y_one_hot=F.one_hot(input_ids, num_classes=model.config.vocab_size)\n",
    "correct_next_token_probs = (my_probs[:,:-1]*y_one_hot[:,1:]).sum(-1) #I'm sure there's waaay more efficient ways to do this\n",
    "my_loss=-torch.log(correct_next_token_probs).mean()\n",
    "print(my_loss.item(), outputs.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0affa34f-2533-4a06-aa89-eb3430149250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3751845359802246 3.3751840591430664 0.9376922065287221\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "y_one_hot=F.one_hot(input_ids, num_classes=model.config.vocab_size)\n",
    "correct_next_token_probs = (my_probs[:,:-1]*y_one_hot[:,1:]).sum(-1) #I'm sure there's waaay more efficient ways to do this\n",
    "my_loss=-torch.log(correct_next_token_probs).mean()\n",
    "\n",
    "paris_only_loss=-np.log(my_probs[0, 5, 12366].item())\n",
    "print(my_loss.item(), outputs.loss.item(), paris_only_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5c2a14-78dc-425e-9bcf-481bfbebe430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12366 0.39153  Paris\n",
      "264 0.08419  a\n",
      "279 0.0704  the\n",
      "832 0.03096  one\n",
      "1101 0.03061  also\n",
      "2162 0.02528  home\n",
      "3967 0.02462  known\n",
      "539 0.01659  not\n",
      "459 0.01241  an\n",
      "7559 0.01172  located\n"
     ]
    }
   ],
   "source": [
    "sI=np.argsort(my_probs[0,5, :].detach().cpu().float().numpy())[::-1]\n",
    "for i in sI[:10]:\n",
    "    print(i, round(my_probs[0, 5, i].item(),5), tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9796958-4034-4f8f-83a5-d821772fdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='pretrained_'\n",
    "filtered_params = [(name, p) for name, p in model.named_parameters() if p.requires_grad]\n",
    "# layers_name='all'\n",
    "\n",
    "layers_name='first_8'\n",
    "filtered_params = filtered_params[1:73] \n",
    "\n",
    "# layers_name='last_8'\n",
    "# filtered_params = filtered_params[73:] #Last 8 layers - some nice structue, but yeah more parabolic than I would like\n",
    "\n",
    "random_seed_1=11\n",
    "random_seed_2=111\n",
    "\n",
    "# Generate and normalize random directions\n",
    "direction1 = get_random_directions(filtered_params, seed=random_seed_1)\n",
    "direction2 = get_random_directions(filtered_params, seed=random_seed_2)\n",
    "\n",
    "direction1 = normalize_direction(direction1, filtered_params)\n",
    "direction2 = normalize_direction(direction2, filtered_params)\n",
    "\n",
    "original_params = OrderedDict()\n",
    "for name, param in filtered_params:\n",
    "    original_params[name] = param.data.clone()\n",
    "\n",
    "alphas=np.linspace(-2.5, 2.5, num_points)\n",
    "betas=np.linspace(-2.5, 2.5, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a003afd-a67c-4eeb-ab84-23668e94a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06609000-c4c1-4b6e-af82-064078305f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move away from center\n",
    "alpha_shift=-0.9 \n",
    "beta_shift=0.05\n",
    "\n",
    "alphas_shifted=alphas-alpha_shift #Shift scan points to keep thing consistent. \n",
    "betas_shifted=betas-beta_shift\n",
    "\n",
    "#Replace actual model parameters with the shifted ones. \n",
    "for name, param in model.named_parameters():\n",
    "    if name in direction1 and name in direction2:\n",
    "        param.data = original_params[name] + alpha_shift * direction1[name] + beta_shift * direction2[name]\n",
    "\n",
    "#Make copy for scaning/replacing. \n",
    "original_params_shifted = OrderedDict()\n",
    "for name, param in filtered_params:\n",
    "    original_params_shifted[name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e502c6d-2455-4ca3-93b9-692ebe9a1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss= 12.032893424603056 [12366, 5.9e-06, ' Paris'] [37180, 0.0486275, 'adar']\n",
      "1 loss= 11.69476953080568 [12366, 8.3e-06, ' Paris'] [37180, 0.047558, 'adar']\n",
      "2 loss= 11.35649891006215 [12366, 1.17e-05, ' Paris'] [37180, 0.0464448, 'adar']\n",
      "3 loss= 11.019169033754162 [12366, 1.64e-05, ' Paris'] [37180, 0.0452946, 'adar']\n",
      "4 loss= 10.683672679717075 [12366, 2.29e-05, ' Paris'] [37180, 0.0441116, 'adar']\n",
      "5 loss= 10.350730993766888 [12366, 3.2e-05, ' Paris'] [37180, 0.0429004, 'adar']\n",
      "6 loss= 10.020812051325684 [12366, 4.45e-05, ' Paris'] [37180, 0.0416649, 'adar']\n",
      "7 loss= 9.694163320889235 [12366, 6.16e-05, ' Paris'] [37180, 0.0404064, 'adar']\n",
      "8 loss= 9.370973957108722 [12366, 8.52e-05, ' Paris'] [37180, 0.0391275, 'adar']\n",
      "9 loss= 9.05123692036561 [12366, 0.0001172, ' Paris'] [37180, 0.0378297, 'adar']\n",
      "10 loss= 8.734785279797308 [12366, 0.0001609, ' Paris'] [37180, 0.0365111, 'adar']\n",
      "11 loss= 8.421417450904402 [12366, 0.0002201, ' Paris'] [37180, 0.0351736, 'adar']\n",
      "12 loss= 8.110884671537402 [12366, 0.0003003, ' Paris'] [37180, 0.0338192, 'adar']\n",
      "13 loss= 7.802966929477319 [12366, 0.0004085, ' Paris'] [37180, 0.0324519, 'adar']\n",
      "14 loss= 7.4975491984180325 [12366, 0.0005544, ' Paris'] [37180, 0.0310791, 'adar']\n",
      "15 loss= 7.194589500234173 [12366, 0.0007506, ' Paris'] [37180, 0.0297094, 'adar']\n",
      "16 loss= 6.894141120476094 [12366, 0.0010137, ' Paris'] [37180, 0.0283528, 'adar']\n",
      "17 loss= 6.596258632347082 [12366, 0.0013655, ' Paris'] [37180, 0.0270185, 'adar']\n",
      "18 loss= 6.3010211369514435 [12366, 0.0018344, ' Paris'] [37180, 0.0257144, 'adar']\n",
      "19 loss= 6.00849115987506 [12366, 0.0024578, ' Paris'] [37180, 0.0244478, 'adar']\n",
      "20 loss= 5.718735244527697 [12366, 0.0032839, ' Paris'] [37180, 0.0232235, 'adar']\n",
      "21 loss= 5.431801604495392 [12366, 0.0043752, ' Paris'] [11, 0.0246583, ',']\n",
      "22 loss= 5.147767156808337 [12366, 0.0058124, ' Paris'] [11, 0.0270032, ',']\n",
      "23 loss= 4.866750755380332 [12366, 0.0076983, ' Paris'] [11, 0.0292877, ',']\n",
      "24 loss= 4.588929410671712 [12366, 0.0101637, ' Paris'] [11, 0.0314618, ',']\n",
      "25 loss= 4.314519369502402 [12366, 0.013373, ' Paris'] [11, 0.0334772, ',']\n",
      "26 loss= 4.043858165204437 [12366, 0.0175297, ' Paris'] [11, 0.035288, ',']\n",
      "27 loss= 3.777378590900919 [12366, 0.0228826, ' Paris'] [11, 0.0368543, ',']\n",
      "28 loss= 3.515636059332532 [12366, 0.0297289, ' Paris'] [11, 0.0381438, ',']\n",
      "29 loss= 3.259315711609584 [12366, 0.0384147, ' Paris'] [11, 0.0391271, ',']\n",
      "30 loss= 3.0092514132101837 [12366, 0.0493286, ' Paris'] [12366, 0.0493286, ' Paris']\n",
      "31 loss= 2.76641300138025 [12366, 0.0628872, ' Paris'] [12366, 0.0628872, ' Paris']\n",
      "32 loss= 2.531904110406872 [12366, 0.0795075, ' Paris'] [12366, 0.0795075, ' Paris']\n",
      "33 loss= 2.306936506630405 [12366, 0.0995658, ' Paris'] [12366, 0.0995658, ' Paris']\n",
      "34 loss= 2.0927530986783647 [12366, 0.1233471, ' Paris'] [12366, 0.1233471, ' Paris']\n",
      "35 loss= 1.890624214584657 [12366, 0.1509775, ' Paris'] [12366, 0.1509775, ' Paris']\n",
      "36 loss= 1.701700082809529 [12366, 0.1823732, ' Paris'] [12366, 0.1823732, ' Paris']\n",
      "37 loss= 1.5269753345786585 [12366, 0.2171916, ' Paris'] [12366, 0.2171916, ' Paris']\n",
      "38 loss= 1.367147735609718 [12366, 0.2548328, ' Paris'] [12366, 0.2548328, ' Paris']\n",
      "39 loss= 1.2225664685654327 [12366, 0.2944734, ' Paris'] [12366, 0.2944734, ' Paris']\n",
      "40 loss= 1.0931886359052676 [12366, 0.3351461, ' Paris'] [12366, 0.3351461, ' Paris']\n",
      "41 loss= 0.9785727847152635 [12366, 0.3758471, ' Paris'] [12366, 0.3758471, ' Paris']\n",
      "42 loss= 0.8779527082234289 [12366, 0.415633, ' Paris'] [12366, 0.415633, ' Paris']\n",
      "43 loss= 0.7902840683587125 [12366, 0.4537159, ' Paris'] [12366, 0.4537159, ' Paris']\n",
      "44 loss= 0.7143691405334077 [12366, 0.4895008, ' Paris'] [12366, 0.4895008, ' Paris']\n",
      "45 loss= 0.6489311795572664 [12366, 0.522604, ' Paris'] [12366, 0.522604, ' Paris']\n",
      "46 loss= 0.5926989513475145 [12366, 0.5528332, ' Paris'] [12366, 0.5528332, ' Paris']\n",
      "47 loss= 0.5444562453207379 [12366, 0.5801572, ' Paris'] [12366, 0.5801572, ' Paris']\n",
      "48 loss= 0.5030849628562766 [12366, 0.6046624, ' Paris'] [12366, 0.6046624, ' Paris']\n",
      "49 loss= 0.4675881262245405 [12366, 0.6265115, ' Paris'] [12366, 0.6265115, ' Paris']\n",
      "50 loss= 0.43708509943681695 [12366, 0.6459165, ' Paris'] [12366, 0.6459165, ' Paris']\n",
      "51 loss= 0.41080321736659076 [12366, 0.6631174, ' Paris'] [12366, 0.6631174, ' Paris']\n",
      "52 loss= 0.38807621357747185 [12366, 0.6783606, ' Paris'] [12366, 0.6783606, ' Paris']\n",
      "53 loss= 0.36833294564266705 [12366, 0.6918868, ' Paris'] [12366, 0.6918868, ' Paris']\n",
      "54 loss= 0.3510763307019162 [12366, 0.70393, ' Paris'] [12366, 0.70393, ' Paris']\n",
      "55 loss= 0.33587616520353697 [12366, 0.7147116, ' Paris'] [12366, 0.7147116, ' Paris']\n",
      "56 loss= 0.32236944803237283 [12366, 0.7244305, ' Paris'] [12366, 0.7244305, ' Paris']\n",
      "57 loss= 0.31023391299597963 [12366, 0.7332754, ' Paris'] [12366, 0.7332754, ' Paris']\n",
      "58 loss= 0.2991973773463493 [12366, 0.7414131, ' Paris'] [12366, 0.7414131, ' Paris']\n",
      "59 loss= 0.28902296293114754 [12366, 0.748995, ' Paris'] [12366, 0.748995, ' Paris']\n",
      "60 loss= 0.27951392367169425 [12366, 0.7561512, ' Paris'] [12366, 0.7561512, ' Paris']\n",
      "61 loss= 0.2705027791490037 [12366, 0.7629958, ' Paris'] [12366, 0.7629958, ' Paris']\n",
      "62 loss= 0.2618536664856074 [12366, 0.7696236, ' Paris'] [12366, 0.7696236, ' Paris']\n",
      "63 loss= 0.25345864392393713 [12366, 0.7761118, ' Paris'] [12366, 0.7761118, ' Paris']\n",
      "64 loss= 0.24524082992907795 [12366, 0.7825161, ' Paris'] [12366, 0.7825161, ' Paris']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it]"
     ]
    }
   ],
   "source": [
    "model_outputs=[]\n",
    "for step in range(n_steps):\n",
    "    losses=[]\n",
    "    model.eval();\n",
    "\n",
    "    with torch.no_grad(): #Check current outputs\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "        sI=np.argsort(my_probs[0,5, :].detach().cpu().float().numpy())[::-1]\n",
    "        current_outs=[[12366,  round(my_probs[0, 5, 12366].item(), 7), ' Paris']] #Put paris at top\n",
    "        for i in sI[:10]:\n",
    "            current_outs.append([i, round(my_probs[0, 5, i].item(),7), tokenizer.decode([i])])\n",
    "        model_outputs.append(current_outs)\n",
    "        print(step, 'loss=', -np.log(my_probs[0, 5, 12366].item()), current_outs[0], current_outs[1])\n",
    "\n",
    "    if step>=delayed_viz_start: #Do I want to compute loss landscape at this step?\n",
    "        with torch.no_grad():\n",
    "            for i, alpha in enumerate(tqdm(alphas_shifted)):\n",
    "                losses.append([])\n",
    "                for j, beta in enumerate(betas_shifted):\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if name in direction1:\n",
    "                            param.data = original_params_shifted[name] + alpha * direction1[name] + beta*direction2[name]\n",
    "                    \n",
    "                    outputs = model(input_ids, labels=input_ids)\n",
    "                    my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "                    paris_only_loss=-np.log(my_probs[0, 5, 12366].item()) #Just Paris\n",
    "                    losses[-1].append(paris_only_loss)\n",
    "            \n",
    "            for name, param in model.named_parameters(): # Restore original shifted parameters\n",
    "                if name in original_params: \n",
    "                    param.data.copy_(original_params_shifted[name])\n",
    "        losses=np.array(losses)\n",
    "        np.save(output_dir +'/'+str(step).zfill(3), losses) #Save loss landscape\n",
    "        \n",
    "        plt.clf()\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        contourf = ax.contourf(alphas, betas, losses, 20, cmap='viridis', alpha=0.8)\n",
    "        contour = ax.contour(alphas, betas, losses, 30, colors='white', linewidths=0.5)\n",
    "        plt.scatter(beta_shift, alpha_shift, c='m')\n",
    "        plt.savefig(output_dir +'/'+str(step).zfill(3)+'.png')\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss #Ok not just paris loss here -> not sure how much I'm worried about that\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #After training I need to replace original_params_shifted with the new trained values\n",
    "    original_params_shifted = OrderedDict()\n",
    "    for name, param in filtered_params:\n",
    "        original_params_shifted[name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f22b9-8f1d-47c1-bb77-755e33a4a419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452d4ae-8fe0-47d3-9827-f81b74bd23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(12366)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e952af0-d499-4c91-b4ce-6d9228d9c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476aad31-079a-457f-84d0-f17d22320e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69462656-caa3-483e-b98c-096f3d0d9aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d965c98-9b4a-40fe-b909-8abd90935729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2dda5e-11ab-4a8a-9b6f-de26127e22ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c4637-b8f1-4998-b333-fa16381103b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2c35c-a46a-4cfe-8634-d97cd0dc7750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478fc9e-5313-4415-9671-e5c153201d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53746a-c9e9-4650-b7aa-b2047cdaf6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50092866-1431-44a5-86cb-c4ac22d08e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c305e72-627b-4342-b358-3265849cebff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
