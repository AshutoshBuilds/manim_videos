{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a95cf92-2971-4a7c-8fba-a3d8caf7e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "paris_coords = np.array([\n",
    "    [48.8575, 2.3514], #Center of Paris\n",
    "    [48.8584, 2.2945], # Eiffel Tower\n",
    "    [48.8530, 2.3499], #Notre Dame\n",
    "    [48.8606,  2.3376], #Louvre\n",
    "    [48.8606, 2.3522]  #Centre Pompidou\n",
    "])\n",
    "\n",
    "madrid_coords = np.array([\n",
    "    [40.4167, -3.7033],   # Center of Madrid\n",
    "    [40.4153, -3.6835],   # Retiro Park \n",
    "    [40.4180, -3.7143],   # Royal Palace \n",
    "    [40.4138, -3.6921],   # Prado Museum \n",
    "    [40.4169, -3.7033]   # Puerta del Sol \n",
    "])\n",
    "\n",
    "berlin_coords = np.array([\n",
    "    [52.5200, 13.4050], # Center of Berlin\n",
    "    [52.5163, 13.3777],   # Brandenburg Gate \n",
    "    [52.5169, 13.4019],   # Museum Island \n",
    "    [52.5074, 13.3904],   # Checkpoint Charlie \n",
    "    [52.5251, 13.3694]   # Berlin Central Station \n",
    "])\n",
    "\n",
    "barcelona_coords = np.array([\n",
    "    [41.3874, 2.1686],    # Center of Barcelona\n",
    "    [41.4036, 2.1744],    # Sagrada Familia\n",
    "    [41.3819, 2.1773],    # Gothic Quarter \n",
    "    [41.4145, 2.1527],    # Park GÃ¼ell \n",
    "    [41.3809, 2.1228],    # Camp Nou\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1069bb10-00f8-447c-bce5-fcf2a109bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_to_subtract=np.array([48.8575, 2.3514]) #Center of Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37273089-b5d0-4dd1-9283-f7c7cf42f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed=25\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9033a960-ad7d-4b00-af9e-f11884b389ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (20, 2)\n",
      "Shape of y: (20,)\n"
     ]
    }
   ],
   "source": [
    "# Combine data into one matrix X and labels y \n",
    "X_raw = np.vstack([madrid_coords, paris_coords, berlin_coords, barcelona_coords]) #, brussels_coords, vienna_coords])\n",
    "y = np.array([0, 0, 0, 0, 0,  #  Madrid labels (0)\n",
    "              1, 1, 1, 1, 1,  #  Paris labels (1)\n",
    "              2, 2, 2, 2, 2, # Berlin labels (2)\n",
    "              3, 3, 3, 3, 3]) # Barcelona Labels (3)\n",
    "\n",
    "# Normalize data (simple scaling by dividing by 100, I guess I could do 10)\n",
    "X = X_raw # / 100 - Simple longitude only problem does not seem to require normalization, that's interesting!\n",
    "X = X_raw - mean_to_subtract #np.array([28, 2.2]) #Ok looks like I might need to normalize liek this for this one to converge\n",
    "\n",
    "rI=np.arange(len(y))\n",
    "np.random.shuffle(rI)\n",
    "rI[0]=5\n",
    "rI[11]=13 # A little manual shuffling to match example I showed earlier in the chapter. \n",
    "X=X[rI,:]\n",
    "y=y[rI]\n",
    "X_raw=X_raw[rI,:]\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691eb1fc-4463-4c0c-a233-73650c4e3b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 11,  2, 10,  6, 18, 16,  0,  3,  7,  1, 13, 14,  9, 19, 17,  8,\n",
       "       12, 15,  4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ed037-65e2-4d53-94ea-2db37ddf61fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64033fd5-f403-4b4d-90a8-2e2519b97814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPSModel(nn.Module):\n",
    "    def __init__(self, input_size=2, output_size=4):\n",
    "        super(TinyGPSModel, self).__init__()\n",
    "        self.output = nn.Linear(input_size, output_size) #, bias=False)  # 3 cities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921d86f8-96f2-41e9-8265-9cc8d362998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyGPSModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de578c1a-f569-4dc7-b85a-8eaf3f91836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where to save training heatmaps\n",
    "import os\n",
    "# save_dir='/Users/stephen/Stephencwelch Dropbox/Stephen Welch/welch_labs/backprop2/graphics/to_manim/jun_6_2'\n",
    "save_dir='/Users/stephen/Stephencwelch Dropbox/welch_labs/ai_book/3_backprop_2/graphics/training_heatmaps_2'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787d8896-6be8-4e6b-8a83-e9868b8de33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE='#21409a'\n",
    "RED='#ed1c24'\n",
    "GREEN='#00a14b'\n",
    "CHILL_BROWN='#948979'\n",
    "PURPLE='#7f3f98'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5898eaf1-e01d-481d-bd9a-b8796fb7e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swaggin - need to sync with Sam on exact numbers\n",
    "# min_long=-6.0\n",
    "# max_long=17.0\n",
    "# min_lat=36.0\n",
    "# max_lat=56.0\n",
    "min_long=-6.0\n",
    "max_long=16.5\n",
    "min_lat=38.6\n",
    "max_lat=53.5  \n",
    "num_steps=256\n",
    "heatmap_viz_logit_multiplier=8 #Makes things more winner take all for cleaner logit viz\n",
    "heatmaps=[np.zeros((num_steps, num_steps)) for i in range(8)]\n",
    "\n",
    "for i, lat in enumerate(np.linspace(max_lat, min_lat, num_steps)):\n",
    "    for j, long in enumerate(np.linspace(min_long, max_long, num_steps)):\n",
    "        with torch.no_grad():\n",
    "            logits=model(torch.tensor([lat, long], dtype=torch.float)).detach()\n",
    "            yhat=torch.nn.Softmax(0)(heatmap_viz_logit_multiplier*logits).numpy()\n",
    "\n",
    "        for k in range(4):\n",
    "            heatmaps[k][i,j]=logits.numpy()[k]\n",
    "            heatmaps[k+3][i,j]=yhat[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db3502e-45d3-4a6d-bed2-d97141e61c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "def create_transparent_colormap(color='cyan', name='transparent_to_color'):\n",
    "    # Convert color name to RGBA\n",
    "    base_color = mcolors.to_rgba(color)\n",
    "    \n",
    "    # Create colormap: transparent (alpha=0) to full color (alpha=1)\n",
    "    colors = [(base_color[0], base_color[1], base_color[2], 0),  # transparent\n",
    "              (base_color[0], base_color[1], base_color[2], 1)]  # full color\n",
    "    \n",
    "    n_bins = 256\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(name, colors, N=n_bins)\n",
    "    return cmap\n",
    "\n",
    "# Create the colormap\n",
    "transparent_cyan_cmap = create_transparent_colormap(BLUE)\n",
    "transparent_yellow_cmap = create_transparent_colormap(RED)\n",
    "transparent_green_cmap = create_transparent_colormap(GREEN)\n",
    "transparent_magenta_cmap = create_transparent_colormap(PURPLE)\n",
    "\n",
    "cmaps=[transparent_cyan_cmap, transparent_yellow_cmap, transparent_green_cmap, transparent_magenta_cmap, \n",
    "       transparent_cyan_cmap, transparent_yellow_cmap, transparent_green_cmap, transparent_magenta_cmap]\n",
    "save_names=['_logits_1.png', '_logits_2.png', '_logits_3.png', '_logits_4.png', \n",
    "            '_yhat_1.png', '_yhat_2.png', '_yhat_3.png', '_yhat_4.png']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877d3f7a-39aa-451f-b106-b7faeed6e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10/9, Loss: 1.7696, 'Accuracy: 0.0000\n",
      "Step 20/19, Loss: 0.0932, 'Accuracy: 0.2500\n",
      "Step 30/29, Loss: 1.9138, 'Accuracy: 0.5000\n",
      "Step 40/39, Loss: 0.2676, 'Accuracy: 0.5000\n",
      "Step 50/49, Loss: 1.8297, 'Accuracy: 0.5000\n",
      "Step 60/59, Loss: 0.4453, 'Accuracy: 0.7500\n",
      "Step 70/69, Loss: 1.6298, 'Accuracy: 0.7500\n",
      "Step 80/79, Loss: 0.1628, 'Accuracy: 0.7500\n",
      "Step 90/89, Loss: 1.3998, 'Accuracy: 0.7500\n",
      "Step 100/99, Loss: 0.1237, 'Accuracy: 0.7500\n",
      "Step 110/109, Loss: 1.1775, 'Accuracy: 0.7500\n",
      "Step 120/119, Loss: 0.0903, 'Accuracy: 1.0000\n",
      "Step 130/129, Loss: 0.9768, 'Accuracy: 1.0000\n",
      "Step 140/139, Loss: 0.0681, 'Accuracy: 1.0000\n",
      "Step 150/149, Loss: 0.8058, 'Accuracy: 1.0000\n",
      "Step 160/159, Loss: 0.0556, 'Accuracy: 1.0000\n",
      "Step 170/169, Loss: 0.6652, 'Accuracy: 1.0000\n",
      "Step 180/179, Loss: 0.0467, 'Accuracy: 1.0000\n",
      "Step 190/189, Loss: 0.5522, 'Accuracy: 1.0000\n",
      "Step 200/199, Loss: 0.0399, 'Accuracy: 1.0000\n",
      "Step 210/209, Loss: 0.4623, 'Accuracy: 1.0000\n",
      "Step 220/219, Loss: 0.0346, 'Accuracy: 1.0000\n",
      "Step 230/229, Loss: 0.3912, 'Accuracy: 1.0000\n",
      "Step 240/239, Loss: 0.0304, 'Accuracy: 1.0000\n",
      "Step 250/249, Loss: 0.3346, 'Accuracy: 1.0000\n",
      "Step 260/259, Loss: 0.0269, 'Accuracy: 1.0000\n",
      "Step 270/269, Loss: 0.2892, 'Accuracy: 1.0000\n",
      "Step 280/279, Loss: 0.0240, 'Accuracy: 1.0000\n",
      "Step 290/289, Loss: 0.2525, 'Accuracy: 1.0000\n",
      "Step 300/299, Loss: 0.0216, 'Accuracy: 1.0000\n",
      "Step 310/309, Loss: 0.2225, 'Accuracy: 1.0000\n",
      "Step 320/319, Loss: 0.0196, 'Accuracy: 1.0000\n",
      "Step 330/329, Loss: 0.1976, 'Accuracy: 1.0000\n",
      "Step 340/339, Loss: 0.0178, 'Accuracy: 1.0000\n",
      "Step 350/349, Loss: 0.1769, 'Accuracy: 1.0000\n",
      "Step 360/359, Loss: 0.0163, 'Accuracy: 1.0000\n",
      "Step 370/369, Loss: 0.1594, 'Accuracy: 1.0000\n",
      "Step 380/379, Loss: 0.0150, 'Accuracy: 1.0000\n",
      "Step 390/389, Loss: 0.1444, 'Accuracy: 1.0000\n",
      "Step 400/399, Loss: 0.0138, 'Accuracy: 1.0000\n",
      "Step 410/409, Loss: 0.1316, 'Accuracy: 1.0000\n",
      "Step 420/419, Loss: 0.0128, 'Accuracy: 1.0000\n",
      "Step 430/429, Loss: 0.1205, 'Accuracy: 1.0000\n",
      "Step 440/439, Loss: 0.0119, 'Accuracy: 1.0000\n",
      "Step 450/449, Loss: 0.1109, 'Accuracy: 1.0000\n",
      "Step 460/459, Loss: 0.0111, 'Accuracy: 1.0000\n",
      "Step 470/469, Loss: 0.1024, 'Accuracy: 1.0000\n",
      "Step 480/479, Loss: 0.0104, 'Accuracy: 1.0000\n",
      "Step 490/489, Loss: 0.0949, 'Accuracy: 1.0000\n",
      "Step 500/499, Loss: 0.0097, 'Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = TinyGPSModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "#I can manually initialize and still learns real good? yeah seems like it!\n",
    "# with torch.no_grad():\n",
    "#     model.output.weight[0,0]=1.0\n",
    "#     model.output.weight[1,0]=0.0\n",
    "#     model.output.weight[2,0]=-1.0\n",
    "#     model.output.bias[0]=0\n",
    "#     model.output.bias[1]=0\n",
    "#     model.output.bias[2]=0\n",
    "\n",
    "weights=[]\n",
    "grads=[]\n",
    "xs=[]\n",
    "ys=[]\n",
    "logitss=[]\n",
    "yhats=[]\n",
    "\n",
    "# Training loop\n",
    "for i in range(500):\n",
    "    xs.append(X_raw[i%len(y)])\n",
    "    ys.append(y[i%len(y)])\n",
    "    weights.append(np.concatenate([model.output.weight.detach().numpy().ravel(), model.output.bias.detach().numpy().ravel()]))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #Stochastic - i think this is a better starting point pedagogically. \n",
    "    outputs = model(torch.tensor(X[i%len(y)]).float())\n",
    "    loss = criterion(outputs, torch.tensor(y[i%len(y)])) \n",
    "\n",
    "    logitss.append(outputs.detach().numpy())\n",
    "    yhats.append(torch.nn.Softmax(0)(outputs.detach()).numpy())\n",
    "\n",
    "    #Heatmaps\n",
    "    heatmaps=[np.zeros((num_steps, num_steps)) for i in range(8)]\n",
    "    for j, lat in enumerate(np.linspace(max_lat, min_lat, num_steps)):\n",
    "        for k, long in enumerate(np.linspace(min_long, max_long, num_steps)):\n",
    "            with torch.no_grad():\n",
    "                coords_norm=np.array([lat, long])-mean_to_subtract\n",
    "                logits=model(torch.tensor(coords_norm.ravel(), dtype=torch.float)).detach()\n",
    "                yhat=torch.nn.Softmax(0)(heatmap_viz_logit_multiplier*logits).numpy()\n",
    "    \n",
    "            for l in range(4):\n",
    "                heatmaps[l][j, k]=logits.numpy()[l]\n",
    "                heatmaps[l+4][j,k]=yhat[l]\n",
    "\n",
    "    for l in range(8):\n",
    "        plt.clf()\n",
    "        plt.figure(frameon=False)\n",
    "        ax = plt.Axes(plt.gcf(), [0., 0., 1., 1.])\n",
    "        ax.set_axis_off()\n",
    "        plt.gcf().add_axes(ax)\n",
    "        plt.imshow(heatmaps[l],  cmap=cmaps[l]) #np.rot90(heatmaps[0])) #Wait and see if I need to rotate or transpose\n",
    "        plt.savefig(save_dir+'/'+str(i)+save_names[l], bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    loss.backward()  # backpropagation\n",
    "    grads.append(np.concatenate([model.output.weight.grad.detach().numpy().ravel(), model.output.bias.grad.detach().numpy().ravel()]))\n",
    "    optimizer.step() #\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            logits=model(torch.tensor(X, dtype=torch.float)) \n",
    "            accuracy=(torch.argmax(logits, dim=1)==torch.tensor(y)).sum().item()/len(y)\n",
    "        print(f\"Step {i+1}/{i}, Loss: {loss.item():.4f}, 'Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "weights=np.array(weights)\n",
    "grads=np.array(grads)\n",
    "xs=np.array(xs)\n",
    "ys=np.array(ys)\n",
    "logitss=np.array(logitss)\n",
    "yhats=np.array(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b543ce88-438e-4a73-b2d6-4bf4eadd765d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "117aee9e-e50c-4bee-b124-3750fc33da93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20930496,  0.64028716, -0.66689724,  0.4933215 , -0.14890456,\n",
       "        0.0255919 ,  0.6683024 ,  0.53918844, -0.6991516 , -0.27485383,\n",
       "        0.62002057,  0.41699004], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232bf30-c599-4580-a1ee-a2179905a4db",
   "metadata": {},
   "source": [
    "Hmm need to pass in that paris point i want to get my initial h and ys...Ok i think it should start with this poitn now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f4eef3-687c-408b-a263-e40dc92a51f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6991516 , -0.27485383,  0.62002057,  0.41699004], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b84486-4553-48f4-b46c-56de735f8eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10727436, 0.16397065, 0.40124083, 0.32751414], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46ff096a-286b-490c-a64a-d155b98ab5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_data=np.hstack((xs, ys.reshape(-1, 1), weights, grads, logitss, yhats))\n",
    "np.save('/Users/stephen/Stephencwelch Dropbox/Stephen Welch/welch_labs/backprop2/hackin/cities_2d_book_1', all_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "347758a5-89b0-4e21-86b9-76f88c56df6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c7b4fb2-900a-4399-b960-66b4fab6e791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6588, 11.0263])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b54de-e56e-428c-81b4-240a58cea278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5666213e-ed10-4067-807b-e7c8daae0be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a190e0e-138d-4035-9e29-bc29e2eb78f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6499, 11.039 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[251%len(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2ee5053-c881-4021-aeda-1e6823184b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.013,  5.52 , 10.276,  4.616], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(logitss[251],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d90d1f67-d81c-488b-aa47-20a53e21a8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.     , 0.0085 , 0.98806, 0.00344], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(yhats[251], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1989b67-f8af-45a9-8771-9d0cd9e04b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.196, -0.622,  0.229,  0.297,  0.677,  0.758, -0.51 ,  0.609,\n",
       "       -1.43 ,  1.406, -0.56 , -0.25 ], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(weights[251], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1616625-edc7-4f9b-9fde-e330c59abb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out=model(torch.tensor(X[251%len(y)]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fc8ee5c-87c1-4034-8115-4bde128f768f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.3860,   5.6140,  11.5058,   4.9243])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d705c-961b-4fa3-a4f7-430ab37ad7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b8aca-ae79-4c05-9056-6d2caf15a599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99507562-5fef-4b60-b608-b2551c60967b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1769, -0.9019],\n",
       "        [ 0.4118,  0.1624],\n",
       "        [ 0.7940,  0.8812],\n",
       "        [-0.6729,  0.7363]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "baceb322-ad46-44d4-9f5d-f3ecddd69151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17690872, -0.90194637,  0.41175136,  0.16236812,  0.79398596,\n",
       "        0.8811785 , -0.67285424,  0.73627746], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.weight.detach().numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d70639c-6401-42b6-bd49-37058f59331b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
