{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fe5a2b-a5ed-4b2a-ab26-532724daeec1",
   "metadata": {},
   "source": [
    "## p17 to p27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c8987-7b54-4d65-91a6-a97dde3da95e",
   "metadata": {},
   "source": [
    "### p17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76240d8-b965-4ccc-83cd-1d972bba9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b045d058-4634-4e2a-a1e5-c5d14fcb81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3b33f5-efbc-4daf-98ee-0fa3c7da0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2a7268-48c1-452b-9d4f-c57b7f9208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('me_no_hat_cropped_1.jpeg')\n",
    "text = \"A photo of a man\"\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc8c7c1-cbac-4bb2-a07a-ac0e9db5e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    image_features_man = model.get_image_features(**inputs)\n",
    "    # image_features_man = image_features_man / image_features_man.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d028f7b-cb34-41a8-bc2b-6ca191e2e491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3799, -0.0205, -0.3645,  0.3117, -0.3376, -0.2418,  0.1636,  0.8491,\n",
       "          0.2491,  0.0771], device='cuda:0'),\n",
       " torch.Size([1, 512]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features_man[0,:10], image_features_man.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c227af-6297-4966-9ce7-e529636c91a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=text, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features_man = model.get_text_features(**inputs)\n",
    "    # text_features_man = text_features_man / text_features_man.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1c7901-f7ef-4015-b4f9-cb152334e603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2643,  0.3246, -0.0228,  0.2032, -0.0099, -0.2975, -0.1399, -1.0689,\n",
       "         -0.0102,  0.3058], device='cuda:0'),\n",
       " torch.Size([1, 512]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_man[0,:10], text_features_man.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccb9bc5-1fb0-4472-a15c-8ff1e94c5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('n02123045_1955.jpg')\n",
    "text = \"A photo of a cat\"\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    image_features_cat = model.get_image_features(**inputs)\n",
    "    # image_features_cat = image_features_cat / image_features_cat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "inputs = processor(text=text, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features_cat = model.get_text_features(**inputs)\n",
    "    # text_features_cat = text_features_cat / text_features_cat.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db54d0c-010b-41d9-a1a7-3799f1254356",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('n02099601_7101.jpg')\n",
    "text = \"A photo of a dog\"\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    image_features_dog = model.get_image_features(**inputs)\n",
    "    # image_features_dog = image_features_dog / image_features_dog.norm(dim=-1, keepdim=True)\n",
    "\n",
    "inputs = processor(text=text, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features_dog = model.get_text_features(**inputs)\n",
    "    # text_features_dog = text_features_dog / text_features_dog.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c08596-519f-4d1e-b73d-18b053dc3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarities=np.zeros((3,3))\n",
    "dot_products=np.zeros((3,3))\n",
    "prods=[]\n",
    "\n",
    "for i, (name_text, feature_text) in enumerate(zip(['cat_text', 'dog_text', 'man_text'], \n",
    "                                 [text_features_cat, text_features_dog, text_features_man])):\n",
    "    prods.append([])\n",
    "    for j, (name_image, feature_image) in enumerate(zip(['cat_image', 'dog_image', 'man_image'], \n",
    "                                     [image_features_cat, image_features_dog, image_features_man])):\n",
    "        cos_similarities[i,j]=torch.cosine_similarity(feature_image, feature_text)\n",
    "        \n",
    "        prod=feature_image*feature_text\n",
    "        # prod=(feature_image-ave_image_features)*(feature_text-ave_text_features) #Try removing mean to make differences more clear?\n",
    "        \n",
    "        dot_products[i,j]=prod.sum().item()\n",
    "        prods[-1].append(prod.detach().cpu().numpy().reshape(16,32))\n",
    "        # print(prod.min().item(), prod.max().item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d94522-1bff-4911-9a4f-fd8df7a74743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27838621, 0.19343476, 0.19544804],\n",
       "       [0.22083758, 0.26263022, 0.20560792],\n",
       "       [0.21663226, 0.20955765, 0.25059876]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4613493-b7a8-44fe-a1de-d3078c71f9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.3657341 , 21.1536808 , 21.51506996],\n",
       "       [27.03812408, 29.33909607, 23.12074661],\n",
       "       [27.89419556, 24.62025833, 29.63657379]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63b2188e-62fe-41e0-b3d5-cfa6300d2ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.5075], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features_man.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13414c67-c3b1-4bce-9059-320691b2430b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.2551], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_man.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c4469-3544-4dcf-92d9-8474f637bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3a0ae-ff6c-4588-a1c5-e2fcfe3f782f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4637be3-d19e-4a11-82b4-2dd98975bd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
