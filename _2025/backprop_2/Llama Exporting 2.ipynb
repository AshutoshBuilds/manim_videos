{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecddf37-b96e-4da7-a7d4-e59f09963b25",
   "metadata": {},
   "source": [
    "## Llama Exporting 2 - With Gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb27ab-ef65-4101-8e51-5d8ee057b14b",
   "metadata": {},
   "source": [
    "- Ok need to figure out exactly what to export here - there's a lot of ways to project all this information on to the animation\n",
    "- Starting on my linux machine then will need to move to runpod I think when I get into gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c5c7ec-049a-487a-86e1-b0aff30e38cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.52.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.32.3)\n",
      "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (2.15.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (1.1.2)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.7.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (3.6.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.1)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.3.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (14.0.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.5)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.4.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.19.11)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch) (68.2.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.2.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.11.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.12.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.20.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers matplotlib tqdm huggingface_hub transformer_lens torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d83107-4034-4892-ae9d-39b0c98c0187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec394f2-f2d6-4e22-9e0b-b38e63cf2760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437a20cc36bb43bdb5de512dd45048a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b41b13-f01c-46ec-b8bd-cfbee7a9d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers import pipeline\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76711176-6e78-43ac-953e-6967bf330889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f322029b-6227-42fe-ba47-2e2e985ea9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_dataset_so_cute=[\n",
    "    #jibberish\n",
    "    \"as dflkja sdf\",\n",
    "    \"18 9sdfsf 8sdf8sns\",\n",
    "    \"as dfasdf uowo fof\",\n",
    "    \n",
    "    # Arithmetic (10 examples)\n",
    "    \"2 + 2 = 4\",\n",
    "    \"4 + 3 = 7\",\n",
    "    \"2 + 9 = 11\",\n",
    "    \"11 - 10 = 1\",\n",
    "    \"3 - 6 = -3\",  # Gets this one wrong\n",
    "    \"3 * 6 = 18\",\n",
    "    \"3 * 2 = 6\",\n",
    "    \"8 / 2 = 4\",\n",
    "    \"128 / 32 = 4\",\n",
    "    \"9 * 7 = 63\",\n",
    "    \n",
    "    # Geography facts (8 examples)\n",
    "    \"The capital of France is Paris\",\n",
    "    \"The capital of Japan is Tokyo\",\n",
    "    \"The capital of Brazil is Brasília\",\n",
    "    \"Mount Everest is located in the Himalayas\",\n",
    "    \"The Amazon River flows through South America\",\n",
    "    \"The largest continent is Asia\",\n",
    "    \"The Pacific Ocean borders Asia, Australia, North America, and South America\",\n",
    "    \"The currency of China is the yuan\",\n",
    "    \n",
    "    # Sports facts (8 examples)\n",
    "    \"The Lakers play in Los Angeles\",\n",
    "    \"The World Cup is held every 4 years\",\n",
    "    \"Serena Williams plays tennis\",\n",
    "    \"The Super Bowl happens in February\",\n",
    "    \"A basketball team has 5 players on the court\",\n",
    "    \"The Olympics occur every 2 years\",\n",
    "    \"Tiger Woods is famous for golf\",\n",
    "    \"Cristiano Ronaldo plays soccer\",\n",
    "    \n",
    "    # Code\n",
    "    \"def add_numbers(a, b):\",\n",
    "    \"import numpy as np\",\n",
    "    \"for i in range(\",\n",
    "    \"if x > 0:\",\n",
    "    \"print('Hello')\",\n",
    "    \"class Dog:\",\n",
    "    \"return x + y\",\n",
    "    \"from datetime import datetime\",\n",
    "    \"x = [1, 2, 3\",\n",
    "    \n",
    "    # Creative writing (10 examples)\n",
    "    \"Once upon a time, in a magical forest\",\n",
    "    \"The old wizard looked up at the stars and\",\n",
    "    \"She opened the mysterious door and found\",\n",
    "    \"The dragon's eyes glowed softly as\",\n",
    "    \"In the bustling marketplace, children\",\n",
    "    \"As the sun set behind the mountains,\",\n",
    "    \"The little robot whirred to life and\",\n",
    "    \"Deep in the ocean, a mermaid\",\n",
    "    \"The spaceship landed with a gentle\",\n",
    "    \"Through the mist came the sound of\",\n",
    "\n",
    "    # Logical reasoning/word relationships\n",
    "    \"If all cats are animals, and Fluffy is a cat, then Fluffy is an animal\",\n",
    "    \"Apple is to fruit as carrot is to vegtable\",\n",
    "    \"Hot is the opposite of cold\",\n",
    "    \"Bird is to fly as fish is to swim\",\n",
    "    \"Monday, Tuesday, Wednesday, Thursday\",\n",
    "    \"January comes before February\",\n",
    "\n",
    "    #Indirect Object Identification\n",
    "    \"When John and Mary went to the shops, John gave the bag to Mary\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to James\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to Sid\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to Amy\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to John\",\n",
    "    \"When Tom and James went to the park, James gave the ball to Tom\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to Dan\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to Martin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf0cd62-b0e0-4ef3-ba95-d7f5205bb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model = HookedTransformer.from_pretrained(model_id, device=device) #Transfomer lens version\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0458c5b-7122-4e57-8339-3cb955639a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|begin_of_text|>', 'The', ' capital', ' of', ' France', ' is']\n",
      "Tokenized answer: [' Paris']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.43</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39.15</span><span style=\"font-weight: bold\">% Token: | Paris|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.43\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m39.15\u001b[0m\u001b[1m% Token: | Paris|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.43 Prob: 39.15% Token: | Paris|\n",
      "Top 1th token. Logit: 16.89 Prob:  8.42% Token: | a|\n",
      "Top 2th token. Logit: 16.71 Prob:  7.04% Token: | the|\n",
      "Top 3th token. Logit: 15.89 Prob:  3.10% Token: | one|\n",
      "Top 4th token. Logit: 15.88 Prob:  3.06% Token: | also|\n",
      "Top 5th token. Logit: 15.69 Prob:  2.53% Token: | home|\n",
      "Top 6th token. Logit: 15.66 Prob:  2.46% Token: | known|\n",
      "Top 7th token. Logit: 15.27 Prob:  1.66% Token: | not|\n",
      "Top 8th token. Logit: 14.98 Prob:  1.24% Token: | an|\n",
      "Top 9th token. Logit: 14.92 Prob:  1.17% Token: | located|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Paris'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Paris'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "answer = \" Paris\"\n",
    "utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c0941e-f9cc-4d6e-a2d3-be70d7820d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\"The captial of France is Paris\", return_tensors=\"pt\").to(device)\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# logits, cache = model.run_with_cache(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621ae8e-17a2-4767-ad42-9ac88ae9866b",
   "metadata": {},
   "source": [
    "- Ok, so I think it's worth working out some kinda \"save model state\" method? \n",
    "- Piece it together and then wrap it up in a method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de92e7cb-ded2-4c39-b8dc-51bfe2f6c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021535251289606094\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The capital of France is Paris\", return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "labels = input_ids.clone()\n",
    "labels[:, :-1] = -100  # Mask all but the last token, just learn on this one for now. Do i need this?\n",
    "\n",
    "lr=1e-6\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run the model and cache activations\n",
    "        logits, cache = model.run_with_cache(input_ids)\n",
    "        \n",
    "        # model snapshot here\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_ids)\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d682952-9954-4efd-81f4-91a7891b7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"The captial of France is\"\n",
    "# answer = \" Paris\"\n",
    "# utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2484addb-fcf3-4a28-9fc7-22f81143746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/workspace/jun_1_1'\n",
    "save_name='snapshot_1.p'\n",
    "\n",
    "all_params={n:v for n,v in model.named_parameters()}\n",
    "with torch.no_grad():\n",
    "    logits, cache = model.run_with_cache(input_ids)\n",
    "\n",
    "layer_snapshot={}\n",
    "for layer_num in range(16):\n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "    a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "    layer_snapshot[param_name]= a[0]\n",
    "\n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "    a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "    layer_snapshot[param_name]= a[0]\n",
    "    \n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "    a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "    layer_snapshot[param_name]= a\n",
    "\n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "    a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "    layer_snapshot[param_name]= a\n",
    "\n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "    if all_params[param_name].grad is not None: \n",
    "        a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot[param_name+'.grad']= a[0]\n",
    "    \n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "    if all_params[param_name].grad is not None: \n",
    "        a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot[param_name+'.grad']= a[0]\n",
    "        \n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "    if all_params[param_name].grad is not None: \n",
    "        a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].grad.detach().cpu()).numpy()\n",
    "        layer_snapshot[param_name+'.grad']= a\n",
    "    \n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "    if all_params[param_name].grad is not None: \n",
    "        a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].grad.detach().cpu()).numpy()\n",
    "        layer_snapshot[param_name+'.grad']= a\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.hook_resid_mid'\n",
    "    a=torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    layer_snapshot[cache_name]=a\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.mlp.hook_post'\n",
    "    a= torch.nn.MaxPool1d(kernel_size=240, stride=240)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    layer_snapshot[cache_name]=a\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.hook_mlp_out'\n",
    "    a= torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    layer_snapshot[cache_name]=a\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.attn.hook_pattern'\n",
    "    a=cache[cache_name].detach().cpu().numpy()\n",
    "    layer_snapshot[cache_name]=a\n",
    "    \n",
    "with open(save_dir+'/'+save_name, 'wb') as f:\n",
    "    pickle.dump(layer_snapshot, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bda1c-e34b-4b71-9202-5064da331f69",
   "metadata": {},
   "source": [
    "Ok let me jump into manim, visaulize outputs for this one output and make sure it makes sense, then I'll wrap up the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "708f8c52-b331-4cb4-8294-9054188670d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 7, 7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d9bd3a8-599e-45a6-8a94-45574d04c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in layer_snapshot.items():\n",
    "#     print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149abfac-7e32-4fe5-8199-dd5f491dee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844db9c4-16e5-4a6c-9b73-59dcc86cbf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021b99e-5910-4d5b-a4fb-929f5a6a7e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d4297-75f8-451e-ad66-8998ffba6d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9309b-8d88-4ce1-90ec-7dd21a8b02b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce41b1ba-50d6-4dee-a3a1-3924c9066470",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b5f1b8-049a-4d0a-bfd7-f02ff778587f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 1, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06d38598-1945-4c6f-ad91-1de474abba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for layer_num in range(16):\n",
    "#     param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "#     a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a[0])\n",
    "\n",
    "#     param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "#     a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a[0])\n",
    "    \n",
    "#     param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "#     a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a)\n",
    "\n",
    "#     param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "#     a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a)\n",
    "\n",
    "\n",
    "\n",
    "# for layer_num in range(16):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff441-35dc-4e3f-b66b-5590fe1fa156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380b89c-1de1-43e6-b4d7-1e2a3755278a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df451e5-33ff-4628-86a7-7c27d5020387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef13048-9584-49b6-9dc8-c89a66049e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41557cc0-8b07-4dd3-911c-729608dde2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9dd2a9-2e56-4ff2-a0a7-3baf0181ee9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528d1692-f42d-43d8-9792-910ebb60a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # inputs = tokenizer(baby_dataset_so_cute[example_index], return_tensors=\"pt\").to(device)\n",
    "    # input_ids = inputs[\"input_ids\"]\n",
    "    # print(baby_dataset_so_cute[example_index])\n",
    "    \n",
    "    # labels = input_ids.clone()\n",
    "    # labels[:, :-1] = -100  # Mask all but the last token, just learn on this one for now.\n",
    "    \n",
    "    # lr=1e-6\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # all_attention_patterns=[]\n",
    "    # for i in range(2):\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         # Run the model and cache activations\n",
    "    #         logits, cache = model.run_with_cache(input_ids)\n",
    "            \n",
    "    #         # Extract attention patterns from all layers and heads\n",
    "    #         attention_data = {}\n",
    "    #         for layer_idx in range(model.cfg.n_layers):\n",
    "    #             layer_attention = cache[f'blocks.{layer_idx}.attn.hook_attn_scores']\n",
    "    #             # layer_attention = cache[f'blocks.{layer_idx}.attn.hook_pattern'] #Scores or patterns, what's better to viz?\n",
    "    #             # Shape: [batch, head, seq_len, seq_len]\n",
    "    #             attention_data[f'layer_{layer_idx}'] = layer_attention.cpu().numpy()\n",
    "    #         all_attention_patterns.append(attention_data)\n",
    "        \n",
    "    #     model.train()\n",
    "    #     optimizer.zero_grad()\n",
    "    #     logits = model(input_ids)\n",
    "    #     shift_logits = logits[..., :-1, :].contiguous()\n",
    "    #     shift_labels = labels[..., 1:].contiguous()\n",
    "    #     loss = F.cross_entropy(\n",
    "    #         shift_logits.view(-1, shift_logits.size(-1)),\n",
    "    #         shift_labels.view(-1),\n",
    "    #         ignore_index=-100\n",
    "    #     )\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "        \n",
    "    #     print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298937b8-26f0-4351-98d3-e60d8f1d0dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded7cac-fa46-4fd1-85e2-3896523c6cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641c396-8e38-4e56-ad5e-f527b59e7338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bdf33-571a-483e-bd87-387fc0871fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80b6a72b-e77f-4cd7-8921-c4d1d1609ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [k for k in cache.keys() if 'blocks.3' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6db437a4-55f9-4389-9b58-bf135bbd3a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blocks.3.hook_resid_pre',\n",
       " 'blocks.3.ln1.hook_scale',\n",
       " 'blocks.3.ln1.hook_normalized',\n",
       " 'blocks.3.attn.hook_q',\n",
       " 'blocks.3.attn.hook_k',\n",
       " 'blocks.3.attn.hook_v',\n",
       " 'blocks.3.attn.hook_rot_q',\n",
       " 'blocks.3.attn.hook_rot_k',\n",
       " 'blocks.3.attn.hook_attn_scores',\n",
       " 'blocks.3.attn.hook_pattern',\n",
       " 'blocks.3.attn.hook_z',\n",
       " 'blocks.3.hook_attn_out',\n",
       " 'blocks.3.hook_resid_mid',\n",
       " 'blocks.3.ln2.hook_scale',\n",
       " 'blocks.3.ln2.hook_normalized',\n",
       " 'blocks.3.mlp.hook_pre',\n",
       " 'blocks.3.mlp.hook_pre_linear',\n",
       " 'blocks.3.mlp.hook_post',\n",
       " 'blocks.3.hook_mlp_out',\n",
       " 'blocks.3.hook_resid_post']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0b3d71a-5a3e-43f5-a97f-47bd00dc6619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2048])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.3.hook_resid_mid'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f1b9a6c-cdad-4995-97bb-c1001424d796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8192])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.3.mlp.hook_pre'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e885c7b4-07e7-4d50-83ec-2aac95c0f503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8192])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.3.mlp.hook_pre_linear'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca425a4-0b0f-48ca-9c5b-55c90b546631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8192])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.3.mlp.hook_post'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "862f4f1a-3016-4502-b5b3-b2a10d66e2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2048])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.3.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2678d75-bd5b-4a3b-bb20-0ddc267b5575",
   "metadata": {},
   "source": [
    "- Ok so I need to decide which of the 7 positions to show, that's a bit tricky, let's try the last position!\n",
    "- I think the caches I want are: `blocks.3.hook_resid_mid`, `blocks.3.mlp.hook_post`, and `blocks.3.hook_mlp_out'`\n",
    "- This is just for activations, so downsampling should be actually pretty easy I think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10d36eb5-7460-4a57-b673-6ae9a4863471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2048])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.3.hook_resid_mid'][:,-1].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0203f6e4-89fb-4988-84b1-3d3e34b8931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.,  4.,  6.,  8., 10.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.MaxPool1d(kernel_size=2, stride=2)(torch.tensor([[[1.,2,3,4,5,6,7,8,9,10]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e861151a-ef26-4da9-8fa7-52542bf27dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache['blocks.3.hook_resid_mid'][:,-1].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bae46617-be9a-415c-9bf0-5f9fd58d976b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8192])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.3.mlp.hook_post'][:,-1].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c06b85fe-941d-46b1-b83b-b0c60316dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 34])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.MaxPool1d(kernel_size=240, stride=240)(cache['blocks.3.mlp.hook_post'][:,-1].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fcbd0be-a31b-4244-a614-41bea9f52ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/home/stephen/backprop2/may_31_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4142dc5b-d458-42ce-ab13-24ab15560899",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/stephen/backprop2/may_31_1/blocks.0.hook_resid_mid.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m cache_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(layer_num)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.hook_resid_mid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m a\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMaxPool1d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)(cache[cache_name][:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mcache_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m cache_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(layer_num)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mlp.hook_post\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m a\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMaxPool1d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m240\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m240\u001b[39m)(cache[cache_name][:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:518\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    517\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 518\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/stephen/backprop2/may_31_1/blocks.0.hook_resid_mid.npy'"
     ]
    }
   ],
   "source": [
    "for layer_num in range(16):\n",
    "    # print(cache['blocks.'+str(layer_num)+'.hook_resid_mid'][:,-1].unsqueeze(0).shape)\n",
    "    cache_name='blocks.'+str(layer_num)+'.hook_resid_mid'\n",
    "    a=torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.mlp.hook_post'\n",
    "    a= torch.nn.MaxPool1d(kernel_size=240, stride=240)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.hook_mlp_out'\n",
    "    a= torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14541471-2fee-417d-ae24-7db1acfed124",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a.ravel(), 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261f20c-4c96-41b2-96db-b6cf7bd4235d",
   "metadata": {},
   "source": [
    "- Ok I think that should be good for activations for now -> I can come back and add input and output stuff later - that will be cool!\n",
    "- Ok now how should I pool down weights? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4669-37bb-4cbc-bc9c-b6a79e3ea2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccdd1e-4b40-4bf9-83da-0cc6121be3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params={n:v for n,v in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383a5a9-0932-4c5f-9a37-c10aef59616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['blocks.0.mlp.W_in'].detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70cfd9f-a399-4d7b-8b7b-67913bccae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['blocks.0.mlp.W_out'].detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445bb03-c8dc-4874-956b-aa2a349461b2",
   "metadata": {},
   "source": [
    "Ok cool so I need to get these puppies down to 32x34 and 34x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c14bd1-90ee-4574-a178-a8286b8f046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params['blocks.0.mlp.W_in'].detach().cpu().unsqueeze(0)).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2dd0d3-3411-4ccd-9786-a22f8d08b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool2d(kernel_size=(240,64), stride=(240,64))(all_params['blocks.0.mlp.W_out'].detach().cpu().unsqueeze(0)).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6f188-8032-4d67-b42a-71cce226db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['blocks.0.mlp.W_in'].detach().cpu().unsqueeze(0).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144ea79-03d8-40c2-95a8-c11317141127",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num in range(16):\n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "    a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a[0])\n",
    "\n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "    a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bfd95-3dd3-4b66-a3a2-852245607a7d",
   "metadata": {},
   "source": [
    "```\n",
    "rsync -auv stephen@dev-3:/home/stephen/backprop2/may_31_1 .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2015c4e-2ebd-4124-876c-0efb4dad2404",
   "metadata": {},
   "source": [
    "- Ok, now the most hand-wavy thing I have to figure out here is what weights and grads to show between the mlp and attention layers.\n",
    "- Can i figure something out that might have a chance at giving us a feel for the significance of each attenttion pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87157b5b-7954-41ac-aa69-c733c181df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', \n",
    "          'blocks.3.attn._W_K', 'blocks.3.attn._W_V', 'blocks.3.attn._b_K', 'blocks.3.attn._b_V']:\n",
    "    print(k, all_params[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a36898-d8d1-46e4-a919-76be3308f898",
   "metadata": {},
   "source": [
    "- Hmm for inputs I'm kinda wondering about keys and queries together somehow?\n",
    "- And for ourtput W_0 is the obvoius choice\n",
    "- How do i preserve cardinality with the attention patterns though?\n",
    "- Hmm I guess I should think about which attention patterns to show -> I'm going to show 10 I think\n",
    "- Oh I guess the queries and ouputs are broken down by attention head, that's nice!\n",
    "- Ok let's assume we're going to export all 32 patters and corresponding weights, and decide which ones to shot at render time.\n",
    "- A simple and maybe not insane idea would be to just show queries.\n",
    "- We could get crazy and have 3 connection points per attention pattern and show keys, queries, and values.\n",
    "- Let's start simple with just queries and see how it goes -> it's definitely a pretty big simplification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2505507-316b-4010-9e10-b7325d26a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c9431-cfa3-42e5-9fe7-e5ed9553b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22af18-5a47-4199-9853-96e74a533747",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num in range(16):\n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "    a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a)\n",
    "\n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "    a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b63cd9-6408-40af-bc0d-4907fc8e2242",
   "metadata": {},
   "source": [
    "Ok dope, let me look at saving attention patterns now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2db2a-a019-4596-be8d-ff96e4f5f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=cache['blocks.3.attn.hook_pattern'].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af845c95-5681-4f41-8006-b7fd5f1e65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1d315-609d-4837-86ff-9732ba034032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp=a[0][0]\n",
    "# tmp[tmp==0]=np.nan\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4ce53-b87d-4854-b570-8c5e3c15ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num in range(16):\n",
    "    # print(cache['blocks.'+str(layer_num)+'.hook_resid_mid'][:,-1].unsqueeze(0).shape)\n",
    "    cache_name='blocks.'+str(layer_num)+'.attn.hook_pattern'\n",
    "    a=cache[cache_name].detach().cpu().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139652c-27c0-4b8e-97ed-282d7803d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44b34d-363f-435d-99f1-fc06f4b305ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c3cd4-fa80-4a00-8049-3e1c2a86c7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fd583-83db-4b9d-87ec-b0da2ba75964",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42aeb3f-4ea8-4adb-9d56-3833510b5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[0,0][1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9673220-67c4-412f-8066-4c070ad2a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(0,(12,12))\n",
    "for i in range(32):\n",
    "    fig.add_subplot(6,6,i+1)\n",
    "    plt.imshow(a[0,i][1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1c25c-37a4-43e2-ad75-3672b930e897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996ed1e-e6aa-41eb-bcbc-db45973788e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9de064-4977-4561-b46b-46ed29cc0aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ad1c5-47c0-4e69-821f-4ddae231803f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626ee85-3506-4316-b4eb-e54bd7d75020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533fef2-1ed4-40d2-a412-483d405fc97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340384eb-aa30-47ea-8519-50c631d938b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2efa2-f7b6-450e-9f1c-a8c82c33864e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69469bd-97eb-4bfe-bd3a-6c3bfa938656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93767b03-6311-44a5-85dc-b1ae7218c2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb54826-a7de-4ed4-8108-dda2c92ae035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe342e-d0f4-4f97-90b4-4aabbc844430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa14300-24b9-48ba-b61d-206375fce5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1262a1-4093-450b-bb3a-8055f6b260e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
