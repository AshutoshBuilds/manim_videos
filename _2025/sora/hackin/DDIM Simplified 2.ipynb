{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55b56a3-7963-4b0e-9161-608ed3bcb2e7",
   "metadata": {},
   "source": [
    "## DDIM Simplified 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea62bb-9997-4abc-b214-4d9d520f35d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x79f79d10d4d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stephen/anaconda3/envs/sora/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# from diffusers import StableDiffusion3Pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler, LMSDiscreteScheduler\n",
    "from diffusers import DDPMScheduler, DDIMScheduler\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import retrieve_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3776735-633a-43c6-99e2-79f9f4734dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"stabilityai/stable-diffusion-2\"\n",
    "# model_id = \"stabilityai/stable-diffusion-2-1\" #Pretty nice samples\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"   #Uses epsilon sampler, images are pretty crappy\n",
    "# model_id = \"CompVis/stable-diffusion-v1-4\" #Also pretty crappy\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\" #Maybe less crappy? Yeah not bad! Wwhat's the scheduler? epsilon! How does it do with DDPM?\n",
    "#Ok, xl-base is not bad with DDPM! Now, DDIM? Yep! Ok let's make this the plan then, I think epsilon is going to make life simpler. \n",
    "\n",
    "# Picking a scheduler\n",
    "# scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "# scheduler = EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\") #Woah so different!\n",
    "# scheduler = DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\") \n",
    "# scheduler = LMSDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\") \n",
    "# scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\") \n",
    "scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\") \n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# For stable diffusion xl 1.0\n",
    "# pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "pipe = DiffusionPipeline.from_pretrained(model_id, scheduler=scheduler,torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "pipe.to(\"cuda\");\n",
    "\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce50e74-624b-4b50-9f27-5a6c05bddd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"a photo of an astronaut riding a horse on the moon\"\n",
    "# prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "# prompt=\"A lone tree standing in the middle of a desert, under the harsh light of midday, casting a long shadow on the sand.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200dae3-d2dd-4c49-aea1-a375d6d310e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = pipe(prompt, guidance=5.0).images[0]  \n",
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2211a0a-d333-4d10-bea9-c6a07adcb69e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caea5fb-99d0-4c54-b26d-a1804a8a61cb",
   "metadata": {},
   "source": [
    "Break into `pipeline_stable_diffusion_xl.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182cef77-0449-4e9f-bc4f-6ebf5cca9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of an astronaut riding a horse on the moon\"\n",
    "prompt_2 = None\n",
    "height = None\n",
    "width = None\n",
    "num_inference_steps = 50\n",
    "timesteps  = None\n",
    "sigmas = None\n",
    "denoising_end = None\n",
    "guidance_scale = 5.0\n",
    "negative_prompt = None\n",
    "negative_prompt_2 = None\n",
    "num_images_per_prompt = 1\n",
    "eta = 0.0\n",
    "# generator = None\n",
    "latents = None\n",
    "prompt_embeds = None\n",
    "negative_prompt_embeds= None\n",
    "pooled_prompt_embeds= None\n",
    "negative_pooled_prompt_embeds = None\n",
    "ip_adapter_image = None\n",
    "ip_adapter_image_embeds = None\n",
    "output_type = \"pil\"\n",
    "return_dict = True\n",
    "cross_attention_kwargs = None\n",
    "guidance_rescale = 0.0\n",
    "original_size = None\n",
    "crops_coords_top_left = (0, 0)\n",
    "target_size = None\n",
    "negative_original_size = None\n",
    "negative_crops_coords_top_left = (0, 0)\n",
    "negative_target_size = None\n",
    "clip_skip  = None\n",
    "callback_on_step_end = None\n",
    "callback_on_step_end_tensor_inputs = [\"latents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994897b6-9eb4-4465-a07e-4c30a593c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually let's try fixing the seed. \n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f7e6-1614-4ca9-b383-98d6bf10c85f",
   "metadata": {},
   "source": [
    "## Setup for Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5047e4-4528-46f7-bc4a-bad66a1e2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Default height and width to unet\n",
    "height = height or pipe.default_sample_size * pipe.vae_scale_factor\n",
    "width = width or pipe.default_sample_size * pipe.vae_scale_factor\n",
    "\n",
    "original_size = original_size or (height, width)\n",
    "target_size = target_size or (height, width)\n",
    "\n",
    "pipe._guidance_scale = guidance_scale\n",
    "pipe._guidance_rescale = guidance_rescale\n",
    "pipe._clip_skip = clip_skip\n",
    "pipe._cross_attention_kwargs = cross_attention_kwargs\n",
    "pipe._denoising_end = denoising_end\n",
    "pipe._interrupt = False\n",
    "\n",
    "batch_size = 1\n",
    "device = pipe._execution_device\n",
    "\n",
    "# 3. Encode input prompt\n",
    "lora_scale = (pipe.cross_attention_kwargs.get(\"scale\", None) if pipe.cross_attention_kwargs is not None else None)\n",
    "\n",
    "with torch.no_grad():\n",
    "    (\n",
    "    prompt_embeds,\n",
    "    negative_prompt_embeds,\n",
    "    pooled_prompt_embeds,\n",
    "    negative_pooled_prompt_embeds,\n",
    "    ) = pipe.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        prompt_2=prompt_2,\n",
    "        device=device,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        do_classifier_free_guidance=pipe.do_classifier_free_guidance,\n",
    "        negative_prompt=negative_prompt,\n",
    "        negative_prompt_2=negative_prompt_2,\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        negative_prompt_embeds=negative_prompt_embeds,\n",
    "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "        lora_scale=lora_scale,\n",
    "        clip_skip=pipe.clip_skip,\n",
    "    )\n",
    "\n",
    "    # 4. Prepare timesteps\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(\n",
    "        pipe.scheduler, num_inference_steps, device, timesteps, sigmas\n",
    "    )\n",
    "\n",
    "    # 5. Prepare latent variables\n",
    "    num_channels_latents = pipe.unet.config.in_channels\n",
    "    latents = pipe.prepare_latents(\n",
    "        batch_size * num_images_per_prompt,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        prompt_embeds.dtype,\n",
    "        device,\n",
    "        generator,\n",
    "        latents,\n",
    "    )\n",
    "\n",
    "    # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "    extra_step_kwargs = pipe.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "    # 7. Prepare added time ids & embeddings\n",
    "    add_text_embeds = pooled_prompt_embeds\n",
    "    if pipe.text_encoder_2 is None:\n",
    "        text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
    "    else:\n",
    "        text_encoder_projection_dim = pipe.text_encoder_2.config.projection_dim\n",
    "    \n",
    "    add_time_ids = pipe._get_add_time_ids(\n",
    "        original_size,\n",
    "        crops_coords_top_left,\n",
    "        target_size,\n",
    "        dtype=prompt_embeds.dtype,\n",
    "        text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "    )\n",
    "    if negative_original_size is not None and negative_target_size is not None:\n",
    "        negative_add_time_ids = pipe._get_add_time_ids(\n",
    "            negative_original_size,\n",
    "            negative_crops_coords_top_left,\n",
    "            negative_target_size,\n",
    "            dtype=prompt_embeds.dtype,\n",
    "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "        )\n",
    "    else:\n",
    "        negative_add_time_ids = add_time_ids\n",
    "    \n",
    "    if pipe.do_classifier_free_guidance:\n",
    "        prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "        add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
    "        add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
    "    \n",
    "    prompt_embeds = prompt_embeds.to(device)\n",
    "    add_text_embeds = add_text_embeds.to(device)\n",
    "    add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
    "    \n",
    "    if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "        image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            device,\n",
    "            batch_size * num_images_per_prompt,\n",
    "            pipe.do_classifier_free_guidance,\n",
    "        )\n",
    "\n",
    "    # 8. Denoising loop\n",
    "    num_warmup_steps = max(len(timesteps) - num_inference_steps * pipe.scheduler.order, 0)\n",
    "    \n",
    "    # 8.1 Apply denoising_end\n",
    "    if (\n",
    "        pipe.denoising_end is not None\n",
    "        and isinstance(self.denoising_end, float)\n",
    "        and pipe.denoising_end > 0\n",
    "        and pipe.denoising_end < 1\n",
    "    ):\n",
    "        discrete_timestep_cutoff = int(\n",
    "            round(\n",
    "                pipe.scheduler.config.num_train_timesteps\n",
    "                - (pipe.denoising_end * self.scheduler.config.num_train_timesteps)\n",
    "            )\n",
    "        )\n",
    "        num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
    "        timesteps = timesteps[:num_inference_steps]\n",
    "    \n",
    "    # 9. Optionally get Guidance Scale Embedding\n",
    "    timestep_cond = None\n",
    "    if pipe.unet.config.time_cond_proj_dim is not None:\n",
    "        guidance_scale_tensor = torch.tensor(pipe.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "        timestep_cond = pipe.get_guidance_scale_embedding(\n",
    "            guidance_scale_tensor, embedding_dim=pipe.unet.config.time_cond_proj_dim\n",
    "        ).to(device=device, dtype=latents.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806128e-d150-4f8f-b4a5-ab2d429ee4d1",
   "metadata": {},
   "source": [
    "## Denoising Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5fc88-2782-4025-9d8f-1c457668e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_history=[latents.detach().cpu().numpy()]\n",
    "sampling_coefficients = {'a':[], 'b': [], 'beta_prod_t':[], 'alpha_prod_t':[], 'alpha_prod_t_prev':[], 'sample_direction_scale':[], 'a':[]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    pipe._num_timesteps = len(timesteps)\n",
    "    with pipe.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            if pipe.interrupt:\n",
    "                continue\n",
    "    \n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if pipe.do_classifier_free_guidance else latents\n",
    "    \n",
    "            latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "    \n",
    "            # predict the noise residual\n",
    "            added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
    "            noise_pred = pipe.unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                timestep_cond=timestep_cond,\n",
    "                cross_attention_kwargs=pipe.cross_attention_kwargs,\n",
    "                added_cond_kwargs=added_cond_kwargs,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "    \n",
    "            # perform guidance\n",
    "            if pipe.do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + pipe.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    \n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents_dtype = latents.dtype\n",
    "            # latents = pipe.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "\n",
    "            ## --- Break into pipe.scheduler.step ---- #\n",
    "            model_output = noise_pred\n",
    "            timestep = t\n",
    "            sample = latents\n",
    "            eta = extra_step_kwargs['eta']\n",
    "            use_clipped_model_output = False\n",
    "            generator = generator\n",
    "            variance_noise = None\n",
    "            return_dict = True\n",
    "    \n",
    "            # 1. get previous step value (=t-1)\n",
    "            prev_timestep = timestep - pipe.scheduler.config.num_train_timesteps // pipe.scheduler.num_inference_steps\n",
    "    \n",
    "            # 2. compute alphas, betas\n",
    "            alpha_prod_t = pipe.scheduler.alphas_cumprod[timestep]\n",
    "            alpha_prod_t_prev = pipe.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else pipe.scheduler.final_alpha_cumprod\n",
    "    \n",
    "            beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "            #Add some instrumentation\n",
    "            sampling_coefficients['beta_prod_t'].append(beta_prod_t)\n",
    "            sampling_coefficients['alpha_prod_t'].append(alpha_prod_t)\n",
    "    \n",
    "            # 3. compute predicted original sample from predicted noise also called\n",
    "            # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "            pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "            pred_epsilon = model_output\n",
    "    \n",
    "            # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "            # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "            variance = pipe.scheduler._get_variance(timestep, prev_timestep)\n",
    "            std_dev_t = eta * variance ** (0.5)\n",
    "    \n",
    "            # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "            pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * pred_epsilon\n",
    "    \n",
    "            # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "            prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "\n",
    "            #SW Simplified framework for comparing DDPM and DDIM - replacing block 7, and technically other blocks above\n",
    "            # a = alpha_prod_t_prev ** 0.5 / alpha_prod_t ** 0.5\n",
    "            # b = (1 - alpha_prod_t_prev - std_dev_t**2) ** 0.5 - (alpha_prod_t_prev ** 0.5 * beta_prod_t ** 0.5 / alpha_prod_t ** 0.5)\n",
    "\n",
    "            #Slight perturb:\n",
    "            # perturb=0.01\n",
    "            # a=a*(1-perturb)\n",
    "            # b=b*(1+perturb)\n",
    "            \n",
    "            # prev_sample = a*sample+b*model_output\n",
    "            # torch.allclose(prev_sample, my_pred_prev_sample)  #Passes\n",
    "            # sampling_coefficients['a'].append(a)\n",
    "            # sampling_coefficients['b'].append(b) \n",
    "\n",
    "            sampling_coefficients['alpha_prod_t_prev'].append(alpha_prod_t_prev)\n",
    "            sampling_coefficients['sample_direction_scale'].append((1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5))\n",
    "\n",
    "            latents=prev_sample\n",
    "            \n",
    "            ## ---- End pipe.scheduler ---- ##\n",
    "            latents_history.append(latents.detach().cpu().numpy())\n",
    "            \n",
    "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipe.scheduler.order == 0):\n",
    "                progress_bar.update()\n",
    "                \n",
    "latents_history=np.array(latents_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa288bf8-69ed-411f-8007-bf1b89844223",
   "metadata": {},
   "source": [
    "### DDIM Equation 12\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\sqrt{\\alpha_{t-1}} \\bigg( \\frac{x_t - \\sqrt{1-\\alpha_t} \\epsilon_\\theta ^{(t)}(x_t)}{\\sqrt{\\alpha_t}} \\bigg)\n",
    "+ \\sqrt{1-\\alpha_{t-1}-\\sigma_t^2} \\cdot \\epsilon_\\theta^{(t)} (x_t) + \\sigma_t \\epsilon_t\n",
    "$$\n",
    "\n",
    "```\n",
    "pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "pred_epsilon = model_output\n",
    "```\n",
    "\n",
    "- Ok so this is computing that first term in the DDIM sampling equation \"predicted $x_0$\"\n",
    "- I wonder if there's some intuition to be had in the video about skipping steps. \n",
    "\n",
    "```\n",
    "pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * pred_epsilon\n",
    "```\n",
    "- Ok, so this is the second term, that seems fine. `std_dev_t` is zero when eta is zero (deterministic)\n",
    "\n",
    "\n",
    "```\n",
    "prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "```\n",
    "- Ok yeah and this guy just adds them together\n",
    "- Overall looks like a pretty straightforward application of the equation from the paper, matches much better than hf ddpm sampler\n",
    "- Ok, comparing to DDPM equations is not that illumating to me right now\n",
    "- Ok how does this compare to DDPM and map to Nakkiran's formulation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4b5e6-ca4f-4887-8c2c-94a0cff866d0",
   "metadata": {},
   "source": [
    "$$\n",
    "x_{t-\\Delta t} \\leftarrow x_t+ \\lambda (f_\\theta (x_t, t) - x_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3636c-b223-4494-95ba-f9fc18201b78",
   "metadata": {},
   "source": [
    "- Ok so this formulation honestly looks pretty different right?\n",
    "- I'm sure there's some rearrangment I can do to make these match - but seems pretty messy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ef4dc-fb44-4452-b193-311ce73108ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf1768-db20-4525-8091-86a9f6c8bf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95cdad2-cb73-4661-814a-7c5cfa126af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365af41-e976-4080-aa87-5dba2fb8d0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2831e-2f65-4d2a-a10d-bf389c0b8359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47acd8a-8280-4ebe-aa58-4406de04952f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77505b98-eb8c-47f6-918a-594d89660b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bae503b-cb3f-44e5-95a2-284586ad73c1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d34fa-ab5f-4a96-a7ab-3862d68f1b08",
   "metadata": {},
   "source": [
    "- Ok first question -> are things going to cancel out in the same way we saw with DDPM and it just collapses to weighted average?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d5db2-891e-46d3-a3d1-3b0cc324150c",
   "metadata": {},
   "source": [
    "```\n",
    "prev_sample = A * pred_original_sample + pred_sample_direction\n",
    "prev_sample = A * pred_original_sample +  B * pred_epsilon\n",
    "prev_sample = A * (sample - C * model_output) / D  +  B * model_output\n",
    "prev_sample = A * sample / D  - A * C * model_output / D  +  B * model_output\n",
    "prev_sample = A * sample / D  - (A * C  / D  -  B) * model_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471c68a-8140-4e21-9475-f0cd90caf2c8",
   "metadata": {},
   "source": [
    "Yeah I can't quite tell what the signs are, but yeah just as average again, where's the magic lol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c623546-b76d-4e1e-9214-bce6e64ecde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(sampling_coefficients['a'])\n",
    "# plt.plot(sampling_coefficients['b'])\n",
    "# plt.legend(['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791a3f-6403-4a90-9a9a-3cd4f111e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_history.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e673e-7065-4f47-bd50-f177cf393294",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(latents_history[:, 0, :, 0, 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965907bc-0325-4ec1-b2bb-676eab3a2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(128):\n",
    "    plt.plot(latents_history[:, 0, :, i, 50], alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696116f-254d-444a-957a-93e256db089b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e5dea-1b17-4284-86c9-a31f2999583c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06fba52-787d-44d6-beae-3f37138f1522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534db27c-6eae-4ee3-86cd-12fb3a32a3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1993a-3411-4045-8668-30f355e53006",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampling_coefficients['beta_prod_t'])\n",
    "plt.plot(sampling_coefficients['alpha_prod_t'])\n",
    "plt.plot(sampling_coefficients['alpha_prod_t_prev'])\n",
    "plt.plot(sampling_coefficients['sample_direction_scale'])\n",
    "plt.legend(['beta_prod_t', 'alpha_prod_t', 'alpha_prod_t_prev', 'sample_direction_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99803526-3a06-48e5-887a-62ec732d33bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37521fd-0111-4b76-b295-d4ef2ae54f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    if not output_type == \"latent\":\n",
    "        # make sure the VAE is in float32 mode, as it overflows in float16\n",
    "        needs_upcasting = pipe.vae.dtype == torch.float16 and pipe.vae.config.force_upcast\n",
    "    \n",
    "        if needs_upcasting:\n",
    "            pipe.upcast_vae()\n",
    "            latents = latents.to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "        elif latents.dtype != pipe.vae.dtype:\n",
    "            if torch.backends.mps.is_available():\n",
    "                # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n",
    "                pipe.vae = pipe.vae.to(latents.dtype)\n",
    "    \n",
    "        # unscale/denormalize the latents\n",
    "        # denormalize with the mean and std if available and not None\n",
    "        has_latents_mean = hasattr(pipe.vae.config, \"latents_mean\") and pipe.vae.config.latents_mean is not None\n",
    "        has_latents_std = hasattr(pipe.vae.config, \"latents_std\") and pipe.vae.config.latents_std is not None\n",
    "        if has_latents_mean and has_latents_std:\n",
    "            latents_mean = (\n",
    "                torch.tensor(pipe.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "            )\n",
    "            latents_std = (\n",
    "                torch.tensor(pipe.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
    "            )\n",
    "            latents = latents * latents_std / pipe.vae.config.scaling_factor + latents_mean\n",
    "        else:\n",
    "            latents = latents / pipe.vae.config.scaling_factor\n",
    "    \n",
    "        image = pipe.vae.decode(latents, return_dict=False)[0]\n",
    "    \n",
    "        # cast back to fp16 if needed\n",
    "        if needs_upcasting:\n",
    "            pipe.vae.to(dtype=torch.float16)\n",
    "\n",
    "        image = pipe.image_processor.postprocess(image, output_type=output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c58ab-93f2-47c3-8485-94ffd18ccdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ae27b-a7e2-48fa-86c6-57e0ac79a187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349ef81-b553-4411-821a-c00352610ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efef8c0-8204-446a-a949-7e5adf5c4da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f0820-174a-4001-bdb9-d2aed5df4eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfc604-a33d-4acb-9315-0e8f7c65ca78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273ea47-1626-47ef-b91d-bb98a19094cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f97be4-3c25-44ad-ab73-ff8b645d989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(0,(10,10))\n",
    "fig.add_subplot(2,2,1); plt.imshow(latents.detach().cpu().numpy()[0,0])\n",
    "fig.add_subplot(2,2,2); plt.imshow(latents.detach().cpu().numpy()[0,1])\n",
    "fig.add_subplot(2,2,3); plt.imshow(latents.detach().cpu().numpy()[0,2])\n",
    "fig.add_subplot(2,2,4); plt.imshow(latents.detach().cpu().numpy()[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe2972-db2a-4858-8d8a-65f9fd11dcff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df3515-23d1-4506-9d74-2796ffe9cf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
