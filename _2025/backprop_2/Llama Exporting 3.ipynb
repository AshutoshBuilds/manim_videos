{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecddf37-b96e-4da7-a7d4-e59f09963b25",
   "metadata": {},
   "source": [
    "## Llama Exporting 3 - with inputs and outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb27ab-ef65-4101-8e51-5d8ee057b14b",
   "metadata": {},
   "source": [
    "- Ok need to figure out exactly what to export here - there's a lot of ways to project all this information on to the animation\n",
    "- Starting on my linux machine then will need to move to runpod I think when I get into gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c5c7ec-049a-487a-86e1-b0aff30e38cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers matplotlib tqdm huggingface_hub transformer_lens torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec394f2-f2d6-4e22-9e0b-b38e63cf2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b41b13-f01c-46ec-b8bd-cfbee7a9d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers import pipeline\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76711176-6e78-43ac-953e-6967bf330889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d670a3bd-d0e9-41d1-817c-0ef176275adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_snapshot(all_params, cache, input_ids, logits, save_dir, save_name):\n",
    "    layer_snapshot={}\n",
    "    for layer_num in range(16):\n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "        a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot[param_name]= a[0]\n",
    "    \n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "        a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot[param_name]= a[0]\n",
    "        \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "        a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "        layer_snapshot[param_name]= a\n",
    "    \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "        a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "        layer_snapshot[param_name]= a\n",
    "    \n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a[0]\n",
    "        \n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a[0]\n",
    "            \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].grad.detach().cpu()).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a\n",
    "        \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].grad.detach().cpu()).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.hook_resid_mid'\n",
    "        a=torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.mlp.hook_post'\n",
    "        a= torch.nn.MaxPool1d(kernel_size=240, stride=240)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.hook_mlp_out'\n",
    "        a= torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.attn.hook_pattern'\n",
    "        a=cache[cache_name].detach().cpu().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "\n",
    "    \n",
    "    # a=torch.nn.MaxPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    a=torch.nn.AvgPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    layer_snapshot['embed.W_E']=a\n",
    "    \n",
    "    # a=torch.nn.MaxPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    a=torch.nn.AvgPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    layer_snapshot['unembed.W_U']=a\n",
    "\n",
    "    if all_params['embed.W_E'].grad is not None: \n",
    "        # a=torch.nn.MaxPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        a=torch.nn.AvgPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot['embed.W_E.grad']=a\n",
    "\n",
    "    if all_params['unembed.W_U'].grad is not None: \n",
    "        # a=torch.nn.MaxPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        a=torch.nn.AvgPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot['unembed.W_U.grad']=a\n",
    "    \n",
    "    prompt_tokens=[tokenizer.decode([i]) for i in input_ids.tolist()[0][1:]]\n",
    "    layer_snapshot['prompt.tokens']=prompt_tokens\n",
    "    a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['embed.W_E'][index, :].detach().cpu().unsqueeze(0)).numpy() \n",
    "                  for index in input_ids.view(-1).tolist()])\n",
    "    layer_snapshot['prompt.embed.W_E']=a\n",
    "\n",
    "    if all_params['embed.W_E'].grad is not None: \n",
    "        a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['embed.W_E'].grad[index, :].detach().cpu().unsqueeze(0)).numpy() \n",
    "                      for index in input_ids.view(-1).tolist()])\n",
    "        layer_snapshot['prompt.embed.W_E.grad']=a\n",
    "    \n",
    "    my_probs=F.softmax(logits, dim=-1)\n",
    "    topk_indices=np.argsort(my_probs[0,-2, :].detach().cpu().float().numpy())[::-1][:40]\n",
    "    topk_tokens=[tokenizer.decode([i]) for i in topk_indices]\n",
    "    topk_probs=[round(my_probs[0, -2, i].item(),6) for i in topk_indices]\n",
    "    layer_snapshot['topk.indices']=topk_indices\n",
    "    layer_snapshot['topk.tokens']=topk_tokens\n",
    "    layer_snapshot['topk.probs']=topk_probs\n",
    "    \n",
    "    a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['unembed.W_U'][:, index].detach().cpu().unsqueeze(0)).numpy() \n",
    "                  for index in topk_indices])\n",
    "    layer_snapshot['topk.unembed.W_U']=a\n",
    "    \n",
    "    if all_params['unembed.W_U'].grad is not None: \n",
    "        a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['unembed.W_U'].grad[:, index].detach().cpu().unsqueeze(0)).numpy() \n",
    "                      for index in topk_indices])\n",
    "        layer_snapshot['topk.unembed.W_U.grad']=a\n",
    "\n",
    "    with open(save_dir+'/'+save_name, 'wb') as f:\n",
    "        pickle.dump(layer_snapshot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f322029b-6227-42fe-ba47-2e2e985ea9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_dataset_so_cute=[\n",
    "    # Geography facts (8 examples)\n",
    "    \"The capital of France is Paris\",\n",
    "    \"The capital of Germany is Berlin\",\n",
    "    \"The capital of Spain is Madrid\",\n",
    "    \"The capital of Japan is Tokyo\",\n",
    "    \"The capital of Brazil is Brasília\",\n",
    "    \"Mount Everest is located in the Himalayas\",\n",
    "    \"The Amazon River flows through South America\",\n",
    "    \"The largest continent is Asia\",\n",
    "    \"The Pacific Ocean borders Asia, Australia, North America, and South America\",\n",
    "    \"The currency of China is the yuan\",\n",
    "    \n",
    "    # Sports facts (8 examples)\n",
    "    \"The Lakers play in Los Angeles\",\n",
    "    \"The World Cup is held every 4 years\",\n",
    "    \"Serena Williams plays tennis\",\n",
    "    \"The Super Bowl happens in February\",\n",
    "    \"A basketball team has 5 players on the court\",\n",
    "    \"The Olympics occur every 2 years\",\n",
    "    \"Tiger Woods is famous for golf\",\n",
    "    \"Cristiano Ronaldo plays soccer\",\n",
    "\n",
    "    \n",
    "    # Arithmetic (10 examples)\n",
    "    \"2 + 2 = 4\",\n",
    "    \"4 + 3 = 7\",\n",
    "    \"2 + 9 = 11\",\n",
    "    \"11 - 10 = 1\",\n",
    "    \"3 - 6 = -3\",  # Gets this one wrong\n",
    "    \"3 * 6 = 18\",\n",
    "    \"3 * 2 = 6\",\n",
    "    \"8 / 2 = 4\",\n",
    "    \"128 / 32 = 4\",\n",
    "    \"9 * 7 = 63\",\n",
    "\n",
    "    \n",
    "    # Code\n",
    "    \"def add_numbers(a, b):\",\n",
    "    \"import numpy as np\",\n",
    "    \"for i in range(\",\n",
    "    \"if x > 0:\",\n",
    "    \"print('Hello')\",\n",
    "    \"class Dog:\",\n",
    "    \"return x + y\",\n",
    "    \"from datetime import datetime\",\n",
    "    \"x = [1, 2, 3\",\n",
    "    \n",
    "    # Creative writing (10 examples)\n",
    "    \"Once upon a time, in a magical forest\",\n",
    "    \"The old wizard looked up at the stars and\",\n",
    "    \"She opened the mysterious door and found\",\n",
    "    \"The dragon's eyes glowed softly as\",\n",
    "    \"In the bustling marketplace, children\",\n",
    "    \"As the sun set behind the mountains,\",\n",
    "    \"The little robot whirred to life and\",\n",
    "    \"Deep in the ocean, a mermaid\",\n",
    "    \"The spaceship landed with a gentle\",\n",
    "    \"Through the mist came the sound of\",\n",
    "\n",
    "    # Logical reasoning/word relationships\n",
    "    \"If all cats are animals, and Fluffy is a cat, then Fluffy is an animal\",\n",
    "    \"Apple is to fruit as carrot is to vegtable\",\n",
    "    \"Hot is the opposite of cold\",\n",
    "    \"Bird is to fly as fish is to swim\",\n",
    "    \"Monday, Tuesday, Wednesday, Thursday\",\n",
    "    \"January comes before February\",\n",
    "\n",
    "    #Indirect Object Identification\n",
    "    \"When John and Mary went to the shops, John gave the bag to Mary\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to James\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to Sid\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to Amy\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to John\",\n",
    "    \"When Tom and James went to the park, James gave the ball to Tom\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to Dan\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to Martin\",\n",
    "\n",
    "    #jibberish\n",
    "    \"as dflkja sdf\",\n",
    "    \"18 9sdfsf 8sdf8sns\",\n",
    "    \"as dfasdf uowo fof\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf0cd62-b0e0-4ef3-ba95-d7f5205bb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model = HookedTransformer.from_pretrained(model_id, device=device) #Transfomer lens version\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0458c5b-7122-4e57-8339-3cb955639a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|begin_of_text|>', 'The', ' capital', ' of', ' France', ' is']\n",
      "Tokenized answer: [' Paris']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.43</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39.15</span><span style=\"font-weight: bold\">% Token: | Paris|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.43\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m39.15\u001b[0m\u001b[1m% Token: | Paris|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.43 Prob: 39.15% Token: | Paris|\n",
      "Top 1th token. Logit: 16.89 Prob:  8.42% Token: | a|\n",
      "Top 2th token. Logit: 16.71 Prob:  7.04% Token: | the|\n",
      "Top 3th token. Logit: 15.89 Prob:  3.10% Token: | one|\n",
      "Top 4th token. Logit: 15.88 Prob:  3.06% Token: | also|\n",
      "Top 5th token. Logit: 15.69 Prob:  2.53% Token: | home|\n",
      "Top 6th token. Logit: 15.66 Prob:  2.46% Token: | known|\n",
      "Top 7th token. Logit: 15.27 Prob:  1.66% Token: | not|\n",
      "Top 8th token. Logit: 14.98 Prob:  1.24% Token: | an|\n",
      "Top 9th token. Logit: 14.92 Prob:  1.17% Token: | located|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Paris'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Paris'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "answer = \" Paris\"\n",
    "utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e812aa9-2209-42d8-a7aa-9fa9dc994b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9376980662345886\n"
     ]
    }
   ],
   "source": [
    "prompt=\"The capital of France is Paris\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "labels = input_ids.clone()\n",
    "labels[:, :-1] = -100  # Mask all but the last token, just learn on this one for now. Do i need this?\n",
    "\n",
    "lr=1e-6\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run the model and cache activations\n",
    "        logits, cache = model.run_with_cache(input_ids)\n",
    "        \n",
    "        # model snapshot here\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_ids)\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22876bb-f5f9-4f0b-9554-1ff9499c3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"The capital of France is\"\n",
    "# answer = \" Paris\"\n",
    "# utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70fae4ac-5684-40d6-9b1e-a4b7227eaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/workspace/jun_2_1'\n",
    "save_name='snapshot_2.p'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "all_params={n:v for n,v in model.named_parameters()}\n",
    "with torch.no_grad():\n",
    "    logits, cache = model.run_with_cache(input_ids)\n",
    "\n",
    "save_snapshot(all_params, cache, input_ids, logits, save_dir, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e1cf992-daf4-4cff-93cf-5efe95c474d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.pooling.AvgPool2d"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.AvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acab1d-457a-45e9-805d-59f44af57cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99baf8c8-7b34-4601-8119-041ca290f139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db332ed-0361-44f9-b814-057060dca715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69462c1-7277-46c3-971b-1597f55e91dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846d48e-78dd-4cdd-a490-30e193f58912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0449407-32d5-4928-80c8-1214b3012b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c1dfb-3770-4cec-a104-3d8098f233c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f900e75-4b8d-44fa-8fe9-e61b5a217bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "690f218c-2d81-48d7-9020-f0d7c2284aa8",
   "metadata": {},
   "source": [
    "- Ok so I don't want to just max pool here\n",
    "- Maybe a reasonable thing to do is sample the prompt and do max pool, then put them together somehow at rendering time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35055da3-84fc-43db-a8a3-e8af96ec68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=torch.nn.MaxPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].detach().cpu().unsqueeze(0)).numpy()\n",
    "\n",
    "# a=torch.nn.MaxPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].detach().cpu().unsqueeze(0)).numpy()\n",
    "\n",
    "# a=torch.nn.MaxPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "\n",
    "# a=torch.nn.MaxPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "\n",
    "# prompt_tokens=[tokenizer.decode([i]) for i in input_ids.tolist()[0][1:]]\n",
    "# a=np.array([torch.nn.MaxPool1d(kernel_size=64, stride=64)(all_params['embed.W_E'][index, :].detach().cpu().unsqueeze(0)).numpy() \n",
    "#               for index in input_ids.view(-1).tolist()])\n",
    "\n",
    "# a=np.array([torch.nn.MaxPool1d(kernel_size=64, stride=64)(all_params['embed.W_E'][index, :].grad.detach().cpu().unsqueeze(0)).numpy() \n",
    "#               for index in input_ids.view(-1).tolist()])\n",
    "\n",
    "# my_probs=F.softmax(logits, dim=-1)\n",
    "# topk_indices=np.argsort(my_probs[0,-2, :].detach().cpu().float().numpy())[::-1][:40]\n",
    "# topk_tokens=[tokenizer.decode([i]) for i in topk_indices]\n",
    "# topk_probs=[round(my_probs[0, -2, i].item(),6) for i in topk_indices]\n",
    "\n",
    "# a=np.array([torch.nn.MaxPool1d(kernel_size=64, stride=64)(all_params['unembed.W_U'][:, index].detach().cpu().unsqueeze(0)).numpy() \n",
    "#               for index in topk_indices])\n",
    "\n",
    "# a=np.array([torch.nn.MaxPool1d(kernel_size=64, stride=64)(all_params['unembed.W_U'].grad[:, index].detach().cpu().unsqueeze(0)).numpy() \n",
    "#               for index in topk_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39ec9c-16fc-4743-8032-45de6029330c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64387f98-bba9-4a70-89ca-2dfc394ad4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 128256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_params['unembed.W_U'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b1a40-ae1e-4984-89e4-6ffc69f68979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffda8bab-e8c3-42ba-9d0a-4b46cd7da5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad23d9-2e45-4922-9e1f-695c531a2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72650dd4-a7e4-4676-820f-ead449126e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c27bca-5706-4f60-8aa2-4cfa936b0eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae17fe-a4cf-404e-9d08-b12bf2ab6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837959dd-f4cc-483f-81d0-270a71ef3933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0c3b6-4072-493d-bace-9b2f7c34fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.tolist()[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c6610-f560-4040-b515-793cb4787dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913f8b7-9c1b-4f75-ba69-6bda13ef5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([791])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a27f3-5556-4853-8124-e2d27db2a095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7339c-c3cc-47ab-a6e0-101d0423f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_probs=F.softmax(logits, dim=-1)\n",
    "sI=np.argsort(my_probs[0,-2, :].detach().cpu().float().numpy())[::-1]\n",
    "for i in sI[:10]:\n",
    "    print(i, round(my_probs[0, -2, i].item(),5), tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e6a86-5369-4abd-a085-5fb068c2c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19380a33-7d8d-4694-a1b6-59cc67a9ba97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96546b-98de-48f7-8097-2cde3bac7e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f35f807c-d1fd-4ca0-b5a5-96a78f3263b8",
   "metadata": {},
   "source": [
    "Ok now, how do I do top k for output? I need the logits, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ece33-137b-462b-bfcb-502790881a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b74d43-fbf8-4e6f-af5e-e75d5e76b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535af066-afe3-43d2-b16f-3750d5d3edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['embed.W_E'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ba66d-ba3c-4cd0-8d42-b83df7a9aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['embed.W_E'][791, :].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530a2bc-28b2-4144-839e-bc37a1740412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3d746-395c-454b-b89c-df0257f7ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67dbab7-938f-4f7e-8a1f-e43e1c40032d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f6be3-7c1c-40d1-9471-02e5a09d3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5f8fb-afa3-4cfd-9a09-b98919ea3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "128256/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a65c91-5737-47d2-a350-aac7cc2badb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['embed.W_E'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3232ff6-17ef-4560-967d-c72c513e6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['unembed.W_U'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b78925-3ee0-4138-88f9-cd087699fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13755d9f-a4ca-4d4f-a0ab-52dcca902562",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b7e8a-153a-4504-b559-5c41ac6a0d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a52770-38ca-4d55-ba0b-75888d6c95bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d872f-9d1d-419d-b629-23bda8b6dec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f35b4b-b685-48e6-802c-d6346c7cd108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5dbba-0f88-491e-bb16-a5dd9ebc88cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de88f51-1b54-499f-85b3-6d939d27a904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3173e8a-7109-4088-a32f-8093b6ab068d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161b505-64f1-4190-a38d-aeb7c0414ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5bdf4-649a-48dc-b3bf-65a8a841dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.test_prompt(\"2 + 2 = \", \"4\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0941e-f9cc-4d6e-a2d3-be70d7820d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\"The captial of France is Paris\", return_tensors=\"pt\").to(device)\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# logits, cache = model.run_with_cache(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621ae8e-17a2-4767-ad42-9ac88ae9866b",
   "metadata": {},
   "source": [
    "- Ok, so I think it's worth working out some kinda \"save model state\" method? \n",
    "- Piece it together and then wrap it up in a method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92e7cb-ded2-4c39-b8dc-51bfe2f6c33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d682952-9954-4efd-81f4-91a7891b7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"The captial of France is\"\n",
    "# answer = \" Paris\"\n",
    "# utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484addb-fcf3-4a28-9fc7-22f81143746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/workspace/jun_1_1'\n",
    "save_name='snapshot_1.p'\n",
    "\n",
    "all_params={n:v for n,v in model.named_parameters()}\n",
    "with torch.no_grad():\n",
    "    logits, cache = model.run_with_cache(input_ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bda1c-e34b-4b71-9202-5064da331f69",
   "metadata": {},
   "source": [
    "Ok let me jump into manim, visaulize outputs for this one output and make sure it makes sense, then I'll wrap up the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f8c52-b331-4cb4-8294-9054188670d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bd3a8-599e-45a6-8a94-45574d04c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in layer_snapshot.items():\n",
    "#     print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149abfac-7e32-4fe5-8199-dd5f491dee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844db9c4-16e5-4a6c-9b73-59dcc86cbf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021b99e-5910-4d5b-a4fb-929f5a6a7e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d4297-75f8-451e-ad66-8998ffba6d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9309b-8d88-4ce1-90ec-7dd21a8b02b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41b1ba-50d6-4dee-a3a1-3924c9066470",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5f1b8-049a-4d0a-bfd7-f02ff778587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d38598-1945-4c6f-ad91-1de474abba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for layer_num in range(16):\n",
    "#     param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "#     a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a[0])\n",
    "\n",
    "#     param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "#     a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a[0])\n",
    "    \n",
    "#     param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "#     a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a)\n",
    "\n",
    "#     param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "#     a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "#     np.save(save_dir+'/'+param_name, a)\n",
    "\n",
    "\n",
    "\n",
    "# for layer_num in range(16):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff441-35dc-4e3f-b66b-5590fe1fa156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380b89c-1de1-43e6-b4d7-1e2a3755278a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df451e5-33ff-4628-86a7-7c27d5020387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef13048-9584-49b6-9dc8-c89a66049e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41557cc0-8b07-4dd3-911c-729608dde2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9dd2a9-2e56-4ff2-a0a7-3baf0181ee9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d1692-f42d-43d8-9792-910ebb60a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # inputs = tokenizer(baby_dataset_so_cute[example_index], return_tensors=\"pt\").to(device)\n",
    "    # input_ids = inputs[\"input_ids\"]\n",
    "    # print(baby_dataset_so_cute[example_index])\n",
    "    \n",
    "    # labels = input_ids.clone()\n",
    "    # labels[:, :-1] = -100  # Mask all but the last token, just learn on this one for now.\n",
    "    \n",
    "    # lr=1e-6\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # all_attention_patterns=[]\n",
    "    # for i in range(2):\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         # Run the model and cache activations\n",
    "    #         logits, cache = model.run_with_cache(input_ids)\n",
    "            \n",
    "    #         # Extract attention patterns from all layers and heads\n",
    "    #         attention_data = {}\n",
    "    #         for layer_idx in range(model.cfg.n_layers):\n",
    "    #             layer_attention = cache[f'blocks.{layer_idx}.attn.hook_attn_scores']\n",
    "    #             # layer_attention = cache[f'blocks.{layer_idx}.attn.hook_pattern'] #Scores or patterns, what's better to viz?\n",
    "    #             # Shape: [batch, head, seq_len, seq_len]\n",
    "    #             attention_data[f'layer_{layer_idx}'] = layer_attention.cpu().numpy()\n",
    "    #         all_attention_patterns.append(attention_data)\n",
    "        \n",
    "    #     model.train()\n",
    "    #     optimizer.zero_grad()\n",
    "    #     logits = model(input_ids)\n",
    "    #     shift_logits = logits[..., :-1, :].contiguous()\n",
    "    #     shift_labels = labels[..., 1:].contiguous()\n",
    "    #     loss = F.cross_entropy(\n",
    "    #         shift_logits.view(-1, shift_logits.size(-1)),\n",
    "    #         shift_labels.view(-1),\n",
    "    #         ignore_index=-100\n",
    "    #     )\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "        \n",
    "    #     print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298937b8-26f0-4351-98d3-e60d8f1d0dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded7cac-fa46-4fd1-85e2-3896523c6cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641c396-8e38-4e56-ad5e-f527b59e7338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bdf33-571a-483e-bd87-387fc0871fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6a72b-e77f-4cd7-8921-c4d1d1609ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [k for k in cache.keys() if 'blocks.3' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db437a4-55f9-4389-9b58-bf135bbd3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3d71a-5a3e-43f5-a97f-47bd00dc6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.3.hook_resid_mid'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b9a6c-cdad-4995-97bb-c1001424d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.3.mlp.hook_pre'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885c7b4-07e7-4d50-83ec-2aac95c0f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.3.mlp.hook_pre_linear'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca425a4-0b0f-48ca-9c5b-55c90b546631",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.3.mlp.hook_post'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f4f1a-3016-4502-b5b3-b2a10d66e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.3.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2678d75-bd5b-4a3b-bb20-0ddc267b5575",
   "metadata": {},
   "source": [
    "- Ok so I need to decide which of the 7 positions to show, that's a bit tricky, let's try the last position!\n",
    "- I think the caches I want are: `blocks.3.hook_resid_mid`, `blocks.3.mlp.hook_post`, and `blocks.3.hook_mlp_out'`\n",
    "- This is just for activations, so downsampling should be actually pretty easy I think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d36eb5-7460-4a57-b673-6ae9a4863471",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.3.hook_resid_mid'][:,-1].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203f6e4-89fb-4988-84b1-3d3e34b8931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool1d(kernel_size=2, stride=2)(torch.tensor([[[1.,2,3,4,5,6,7,8,9,10]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861151a-ef26-4da9-8fa7-52542bf27dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache['blocks.3.hook_resid_mid'][:,-1].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae46617-be9a-415c-9bf0-5f9fd58d976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['blocks.3.mlp.hook_post'][:,-1].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b85fe-941d-46b1-b83b-b0c60316dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool1d(kernel_size=240, stride=240)(cache['blocks.3.mlp.hook_post'][:,-1].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcbd0be-a31b-4244-a614-41bea9f52ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir='/home/stephen/backprop2/may_31_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142dc5b-d458-42ce-ab13-24ab15560899",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num in range(16):\n",
    "    # print(cache['blocks.'+str(layer_num)+'.hook_resid_mid'][:,-1].unsqueeze(0).shape)\n",
    "    cache_name='blocks.'+str(layer_num)+'.hook_resid_mid'\n",
    "    a=torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.mlp.hook_post'\n",
    "    a= torch.nn.MaxPool1d(kernel_size=240, stride=240)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)\n",
    "\n",
    "    cache_name='blocks.'+str(layer_num)+'.hook_mlp_out'\n",
    "    a= torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14541471-2fee-417d-ae24-7db1acfed124",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a.ravel(), 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261f20c-4c96-41b2-96db-b6cf7bd4235d",
   "metadata": {},
   "source": [
    "- Ok I think that should be good for activations for now -> I can come back and add input and output stuff later - that will be cool!\n",
    "- Ok now how should I pool down weights? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4669-37bb-4cbc-bc9c-b6a79e3ea2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccdd1e-4b40-4bf9-83da-0cc6121be3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params={n:v for n,v in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383a5a9-0932-4c5f-9a37-c10aef59616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['blocks.0.mlp.W_in'].detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70cfd9f-a399-4d7b-8b7b-67913bccae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['blocks.0.mlp.W_out'].detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445bb03-c8dc-4874-956b-aa2a349461b2",
   "metadata": {},
   "source": [
    "Ok cool so I need to get these puppies down to 32x34 and 34x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c14bd1-90ee-4574-a178-a8286b8f046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params['blocks.0.mlp.W_in'].detach().cpu().unsqueeze(0)).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2dd0d3-3411-4ccd-9786-a22f8d08b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MaxPool2d(kernel_size=(240,64), stride=(240,64))(all_params['blocks.0.mlp.W_out'].detach().cpu().unsqueeze(0)).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f6f188-8032-4d67-b42a-71cce226db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params['blocks.0.mlp.W_in'].detach().cpu().unsqueeze(0).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144ea79-03d8-40c2-95a8-c11317141127",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num in range(16):\n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "    a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a[0])\n",
    "\n",
    "    param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "    a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bfd95-3dd3-4b66-a3a2-852245607a7d",
   "metadata": {},
   "source": [
    "```\n",
    "rsync -auv stephen@dev-3:/home/stephen/backprop2/may_31_1 .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2015c4e-2ebd-4124-876c-0efb4dad2404",
   "metadata": {},
   "source": [
    "- Ok, now the most hand-wavy thing I have to figure out here is what weights and grads to show between the mlp and attention layers.\n",
    "- Can i figure something out that might have a chance at giving us a feel for the significance of each attenttion pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87157b5b-7954-41ac-aa69-c733c181df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', \n",
    "          'blocks.3.attn._W_K', 'blocks.3.attn._W_V', 'blocks.3.attn._b_K', 'blocks.3.attn._b_V']:\n",
    "    print(k, all_params[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a36898-d8d1-46e4-a919-76be3308f898",
   "metadata": {},
   "source": [
    "- Hmm for inputs I'm kinda wondering about keys and queries together somehow?\n",
    "- And for ourtput W_0 is the obvoius choice\n",
    "- How do i preserve cardinality with the attention patterns though?\n",
    "- Hmm I guess I should think about which attention patterns to show -> I'm going to show 10 I think\n",
    "- Oh I guess the queries and ouputs are broken down by attention head, that's nice!\n",
    "- Ok let's assume we're going to export all 32 patters and corresponding weights, and decide which ones to shot at render time.\n",
    "- A simple and maybe not insane idea would be to just show queries.\n",
    "- We could get crazy and have 3 connection points per attention pattern and show keys, queries, and values.\n",
    "- Let's start simple with just queries and see how it goes -> it's definitely a pretty big simplification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2505507-316b-4010-9e10-b7325d26a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c9431-cfa3-42e5-9fe7-e5ed9553b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22af18-5a47-4199-9853-96e74a533747",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num in range(16):\n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "    a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a)\n",
    "\n",
    "    param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "    a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "    np.save(save_dir+'/'+param_name, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b63cd9-6408-40af-bc0d-4907fc8e2242",
   "metadata": {},
   "source": [
    "Ok dope, let me look at saving attention patterns now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2db2a-a019-4596-be8d-ff96e4f5f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=cache['blocks.3.attn.hook_pattern'].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af845c95-5681-4f41-8006-b7fd5f1e65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1d315-609d-4837-86ff-9732ba034032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp=a[0][0]\n",
    "# tmp[tmp==0]=np.nan\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4ce53-b87d-4854-b570-8c5e3c15ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num in range(16):\n",
    "    # print(cache['blocks.'+str(layer_num)+'.hook_resid_mid'][:,-1].unsqueeze(0).shape)\n",
    "    cache_name='blocks.'+str(layer_num)+'.attn.hook_pattern'\n",
    "    a=cache[cache_name].detach().cpu().numpy()\n",
    "    np.save(save_dir+'/'+cache_name, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139652c-27c0-4b8e-97ed-282d7803d64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44b34d-363f-435d-99f1-fc06f4b305ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c3cd4-fa80-4a00-8049-3e1c2a86c7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fd583-83db-4b9d-87ec-b0da2ba75964",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42aeb3f-4ea8-4adb-9d56-3833510b5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[0,0][1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9673220-67c4-412f-8066-4c070ad2a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(0,(12,12))\n",
    "for i in range(32):\n",
    "    fig.add_subplot(6,6,i+1)\n",
    "    plt.imshow(a[0,i][1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1c25c-37a4-43e2-ad75-3672b930e897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996ed1e-e6aa-41eb-bcbc-db45973788e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9de064-4977-4561-b46b-46ed29cc0aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ad1c5-47c0-4e69-821f-4ddae231803f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626ee85-3506-4316-b4eb-e54bd7d75020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533fef2-1ed4-40d2-a412-483d405fc97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340384eb-aa30-47ea-8519-50c631d938b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2efa2-f7b6-450e-9f1c-a8c82c33864e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69469bd-97eb-4bfe-bd3a-6c3bfa938656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93767b03-6311-44a5-85dc-b1ae7218c2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb54826-a7de-4ed4-8108-dda2c92ae035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe342e-d0f4-4f97-90b4-4aabbc844430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa14300-24b9-48ba-b61d-206375fce5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1262a1-4093-450b-bb3a-8055f6b260e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
