{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecddf37-b96e-4da7-a7d4-e59f09963b25",
   "metadata": {},
   "source": [
    "## Llama Exporting 4 - Multiple Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb27ab-ef65-4101-8e51-5d8ee057b14b",
   "metadata": {},
   "source": [
    "- Ok need to figure out exactly what to export here - there's a lot of ways to project all this information on to the animation\n",
    "- Starting on my linux machine then will need to move to runpod I think when I get into gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c5c7ec-049a-487a-86e1-b0aff30e38cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers matplotlib tqdm huggingface_hub transformer_lens torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec394f2-f2d6-4e22-9e0b-b38e63cf2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b41b13-f01c-46ec-b8bd-cfbee7a9d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from transformers import pipeline\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76711176-6e78-43ac-953e-6967bf330889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d670a3bd-d0e9-41d1-817c-0ef176275adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_snapshot(all_params, cache, input_ids, logits, save_dir, save_name):\n",
    "    layer_snapshot={}\n",
    "    for layer_num in range(16):\n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "        a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot[param_name]= a[0]\n",
    "    \n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "        a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot[param_name]= a[0]\n",
    "        \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "        a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "        layer_snapshot[param_name]= a\n",
    "    \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "        a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].detach().cpu()).numpy()\n",
    "        layer_snapshot[param_name]= a\n",
    "    \n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_in'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a=torch.nn.MaxPool2d(kernel_size=(64,240), stride=(64,240))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a[0]\n",
    "        \n",
    "        param_name='blocks.'+str(layer_num)+'.mlp.W_out'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a=torch.nn.MaxPool2d(kernel_size=(240, 64), stride=(240, 64))(all_params[param_name].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a[0]\n",
    "            \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_Q'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a= torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].grad.detach().cpu()).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a\n",
    "        \n",
    "        param_name='blocks.'+str(layer_num)+'.attn.W_O'\n",
    "        if all_params[param_name].grad is not None: \n",
    "            a = torch.nn.MaxPool2d(kernel_size=(64,64), stride=(64,64))(all_params[param_name].grad.detach().cpu()).numpy()\n",
    "            layer_snapshot[param_name+'.grad']= a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.hook_resid_mid'\n",
    "        a=torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.mlp.hook_post'\n",
    "        a= torch.nn.MaxPool1d(kernel_size=240, stride=240)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.hook_mlp_out'\n",
    "        a= torch.nn.MaxPool1d(kernel_size=64, stride=64)(cache[cache_name][:,-1].unsqueeze(0)).cpu().ravel().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "    \n",
    "        cache_name='blocks.'+str(layer_num)+'.attn.hook_pattern'\n",
    "        a=cache[cache_name].detach().cpu().numpy()\n",
    "        layer_snapshot[cache_name]=a\n",
    "\n",
    "    \n",
    "    # a=torch.nn.MaxPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    a=torch.nn.AvgPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    layer_snapshot['embed.W_E']=a\n",
    "    \n",
    "    # a=torch.nn.MaxPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    a=torch.nn.AvgPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].detach().cpu().unsqueeze(0)).numpy()\n",
    "    layer_snapshot['unembed.W_U']=a\n",
    "\n",
    "    if all_params['embed.W_E'].grad is not None: \n",
    "        # a=torch.nn.MaxPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        a=torch.nn.AvgPool2d(kernel_size=(3206,64), stride=(3206,64))(all_params['embed.W_E'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot['embed.W_E.grad']=a\n",
    "\n",
    "    if all_params['unembed.W_U'].grad is not None: \n",
    "        # a=torch.nn.MaxPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        a=torch.nn.AvgPool2d(kernel_size=(64, 3206), stride=(64, 3206))(all_params['unembed.W_U'].grad.detach().cpu().unsqueeze(0)).numpy()\n",
    "        layer_snapshot['unembed.W_U.grad']=a\n",
    "    \n",
    "    prompt_tokens=[tokenizer.decode([i]) for i in input_ids.tolist()[0][1:]]\n",
    "    layer_snapshot['prompt.tokens']=prompt_tokens\n",
    "    a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['embed.W_E'][index, :].detach().cpu().unsqueeze(0)).numpy() \n",
    "                  for index in input_ids.view(-1).tolist()])\n",
    "    layer_snapshot['prompt.embed.W_E']=a\n",
    "\n",
    "    if all_params['embed.W_E'].grad is not None: \n",
    "        a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['embed.W_E'].grad[index, :].detach().cpu().unsqueeze(0)).numpy() \n",
    "                      for index in input_ids.view(-1).tolist()])\n",
    "        layer_snapshot['prompt.embed.W_E.grad']=a\n",
    "    \n",
    "    my_probs=F.softmax(logits, dim=-1)\n",
    "    topk_indices=np.argsort(my_probs[0,-2, :].detach().cpu().float().numpy())[::-1][:40]\n",
    "    topk_tokens=[tokenizer.decode([i]) for i in topk_indices]\n",
    "    topk_probs=[round(my_probs[0, -2, i].item(),6) for i in topk_indices]\n",
    "    layer_snapshot['topk.indices']=topk_indices\n",
    "    layer_snapshot['topk.tokens']=topk_tokens\n",
    "    layer_snapshot['topk.probs']=topk_probs\n",
    "    \n",
    "    a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['unembed.W_U'][:, index].detach().cpu().unsqueeze(0)).numpy() \n",
    "                  for index in topk_indices])\n",
    "    layer_snapshot['topk.unembed.W_U']=a\n",
    "    \n",
    "    if all_params['unembed.W_U'].grad is not None: \n",
    "        a=np.array([torch.nn.AvgPool1d(kernel_size=64, stride=64)(all_params['unembed.W_U'].grad[:, index].detach().cpu().unsqueeze(0)).numpy() \n",
    "                      for index in topk_indices])\n",
    "        layer_snapshot['topk.unembed.W_U.grad']=a\n",
    "\n",
    "    with open(save_dir+'/'+save_name, 'wb') as f:\n",
    "        pickle.dump(layer_snapshot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f322029b-6227-42fe-ba47-2e2e985ea9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_dataset_so_cute=[\n",
    "    # Geography facts (8 examples)\n",
    "    \"The capital of France is Paris\",\n",
    "    \"The capital of Germany is Berlin\",\n",
    "    \"The capital of Spain is Madrid\",\n",
    "    \"The capital of Japan is Tokyo\",\n",
    "    \"The capital of Brazil is BrasÃ­lia\",\n",
    "    \"Mount Everest is located in the Himalayas\",\n",
    "    \"The Amazon River flows through South America\",\n",
    "    \"The largest continent is Asia\",\n",
    "    \"The Pacific Ocean borders Asia, Australia, North America, and South America\",\n",
    "    \"The currency of China is the yuan\",\n",
    "    \n",
    "    # Sports facts (8 examples)\n",
    "    \"The Lakers play in Los Angeles\",\n",
    "    \"The World Cup is held every 4 years\",\n",
    "    \"Serena Williams plays tennis\",\n",
    "    \"The Super Bowl happens in February\",\n",
    "    \"A basketball team has 5 players on the court\",\n",
    "    \"The Olympics occur every 2 years\",\n",
    "    \"Tiger Woods is famous for golf\",\n",
    "    \"Cristiano Ronaldo plays soccer\",\n",
    "\n",
    "    \n",
    "    # Arithmetic (10 examples)\n",
    "    \"2+2 = 4\",\n",
    "    \"4+3= 7\",\n",
    "    \"2+9= 11\",\n",
    "    \"11-10= 1\",\n",
    "    \"3-6= -3\",  # Gets this one wrong\n",
    "    \"3*6= 18\",\n",
    "    \"3*2= 6\",\n",
    "    \"8/2= 4\",\n",
    "    \"128/32= 4\",\n",
    "    \"9*7= 63\",\n",
    "\n",
    "    \n",
    "    # Code\n",
    "    \"def add_numbers(a, b):\",\n",
    "    \"import numpy as np\",\n",
    "    \"for i in range(\",\n",
    "    \"if x > 0:\",\n",
    "    \"print('Hello')\",\n",
    "    \"class Dog:\",\n",
    "    \"return x + y\",\n",
    "    \"from datetime import datetime\",\n",
    "    \"x = [1, 2, 3\",\n",
    "    \n",
    "    # Creative writing (10 examples)\n",
    "    # \"Once upon a time, in a magical forest\",\n",
    "    # \"The old wizard looked up at the stars and\",\n",
    "    # \"She opened the mysterious door and found\",\n",
    "    # \"The dragon's eyes glowed softly as\",\n",
    "    # \"In the bustling marketplace, children\",\n",
    "    # \"As the sun set behind the mountains,\",\n",
    "    # \"The little robot whirred to life and\",\n",
    "    # \"Deep in the ocean, a mermaid\",\n",
    "    # \"The spaceship landed with a gentle\",\n",
    "    # \"Through the mist came the sound of\",\n",
    "\n",
    "    # Logical reasoning/word relationships\n",
    "    \"If all cats are animals, and Fluffy is a cat, then Fluffy is an animal\",\n",
    "    \"Apple is to fruit as carrot is to vegtable\",\n",
    "    \"Hot is the opposite of cold\",\n",
    "    \"Bird is to fly as fish is to swim\",\n",
    "    \"Monday, Tuesday, Wednesday, Thursday\",\n",
    "    \"January comes before February\",\n",
    "\n",
    "    #Indirect Object Identification\n",
    "    \"When John and Mary went to the shops, John gave the bag to Mary\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to James\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to Sid\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to Amy\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to John\",\n",
    "    \"When Tom and James went to the park, James gave the ball to Tom\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to Dan\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to Martin\",\n",
    "\n",
    "    #jibberish\n",
    "    # \"as dflkja sdf\",\n",
    "    # \"18 9sdfsf 8sdf8sns\",\n",
    "    # \"as dfasdf uowo fof\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf0cd62-b0e0-4ef3-ba95-d7f5205bb273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model = HookedTransformer.from_pretrained(model_id, device=device) #Transfomer lens version\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecac12-a0d9-49ba-a90c-53c5143f866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The capital of France is Paris 7\n",
      "0.9376980662345886\n",
      "1 The capital of Germany is Berlin 7\n",
      "0.3606833815574646\n",
      "2 The capital of Spain is Madrid 7\n",
      "0.249673992395401\n",
      "3 The capital of Japan is Tokyo 7\n",
      "0.26188361644744873\n",
      "10 The Lakers play in Los Angeles 7\n",
      "0.004927633795887232\n",
      "13 The Super Bowl happens in February 7\n",
      "1.3404767513275146\n",
      "17 Cristiano Ronaldo plays soccer 7\n",
      "4.926534652709961\n",
      "18 2+2 = 4 7\n",
      "1.0964512825012207\n",
      "19 4+3= 7 7\n",
      "1.5041853189468384\n",
      "20 2+9= 11 7\n",
      "0.9847300052642822\n",
      "21 11-10= 1 7\n",
      "0.5763028860092163\n",
      "22 3-6= -3 7\n",
      "1.7122907638549805\n",
      "23 3*6= 18 7\n",
      "0.16935837268829346\n",
      "24 3*2= 6 7\n",
      "0.6665123105049133\n",
      "25 8/2= 4 7\n",
      "2.3877921104431152\n",
      "26 128/32= 4 7\n",
      "0.08347074687480927\n",
      "27 9*7= 63 7\n",
      "0.053089652210474014\n",
      "31 if x > 0: 7\n"
     ]
    }
   ],
   "source": [
    "save_dir='/workspace/jun_3_1'\n",
    "# save_name='snapshot_2.p'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "lr=1e-7\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "snapshot_count=0\n",
    "for i, prompt in enumerate(baby_dataset_so_cute):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    # print(i, prompt, len(input_ids[0]))\n",
    "    if len(input_ids[0]) != 7: continue #Only fuck with length 7 for now to keep attention patterns in animatins consistent. \n",
    "    print(i, prompt, len(input_ids[0]))\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :-1] = -100  # Mask all but the last token, just learn on this one for now. Do i need this?\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_ids)\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # for i in range(1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run the model and cache activations\n",
    "        logits, cache = model.run_with_cache(input_ids)\n",
    "        \n",
    "        # model snapshot here\n",
    "        all_params={n:v for n,v in model.named_parameters()}\n",
    "        save_snapshot(all_params, cache, input_ids, logits, save_dir, save_name='snapshot_'+str(snapshot_count)+'.p')\n",
    "        snapshot_count+=1\n",
    "\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e35a58-ec8b-47b1-9d5a-f4ad93adabbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9000a-6a61-43e8-86bc-cc2a902685ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69469bd-97eb-4bfe-bd3a-6c3bfa938656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93767b03-6311-44a5-85dc-b1ae7218c2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb54826-a7de-4ed4-8108-dda2c92ae035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe342e-d0f4-4f97-90b4-4aabbc844430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa14300-24b9-48ba-b61d-206375fce5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1262a1-4093-450b-bb3a-8055f6b260e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
