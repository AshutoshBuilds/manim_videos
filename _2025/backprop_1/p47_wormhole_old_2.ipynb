{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56d1131-3818-4959-993e-8a39597af93e",
   "metadata": {},
   "source": [
    "## P47 Wormhole\n",
    "- Ok I might end up using these exports all the way back to like P41, we'll see\n",
    "- First important thing to do is get wormhole idea working here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a72b6a-70cd-4d5c-b04c-6325114699aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers matplotlib tqdm huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918cc7c3-74f2-438c-ad8a-2cfb26f1e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85155a3b-c331-41c0-9402-8c3d05354ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1897e993-77db-4f3f-982f-b66740971ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "# model_id = \"google/gemma-3-1b-pt\"\n",
    "# model_id = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cee2066-a7f0-4371-afaa-e22cb9dc8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Only needed for llama random initialization\n",
    "# config_dict = {\n",
    "#   \"_attn_implementation_autoset\": True,\n",
    "#   \"architectures\": [\n",
    "#     \"LlamaForCausalLM\"\n",
    "#   ],\n",
    "#   \"attention_bias\": False,\n",
    "#   \"attention_dropout\": 0.0,\n",
    "#   \"bos_token_id\": 128000,\n",
    "#   \"eos_token_id\": 128001,\n",
    "#   \"head_dim\": 64,\n",
    "#   \"hidden_act\": \"silu\",\n",
    "#   \"hidden_size\": 2048,\n",
    "#   \"initializer_range\": 0.02,\n",
    "#   \"intermediate_size\": 8192,\n",
    "#   \"max_position_embeddings\": 131072,\n",
    "#   \"mlp_bias\": False,\n",
    "#   \"model_type\": \"llama\",\n",
    "#   \"num_attention_heads\": 32,\n",
    "#   \"num_hidden_layers\": 16,\n",
    "#   \"num_key_value_heads\": 8,\n",
    "#   \"pretraining_tp\": 1,\n",
    "#   \"rms_norm_eps\": 1e-05,\n",
    "#   \"rope_scaling\": {\n",
    "#     \"factor\": 32.0,\n",
    "#     \"high_freq_factor\": 4.0,\n",
    "#     \"low_freq_factor\": 1.0,\n",
    "#     \"original_max_position_embeddings\": 8192,\n",
    "#     \"rope_type\": \"llama3\"\n",
    "#   },\n",
    "#   \"rope_theta\": 500000.0,\n",
    "#   \"tie_word_embeddings\": True,\n",
    "#   \"torch_dtype\": \"float32\",\n",
    "#   \"transformers_version\": \"4.50.3\",\n",
    "#   \"use_cache\": True,\n",
    "#   \"vocab_size\": 128256\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88763c12-4b95-423e-ad59-6d0004ea456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random init\n",
    "# model_config = GPT2Config() #Full sized model\n",
    "# model = GPT2LMHeadModel(model_config).to(device) #Ok i should see what happens with full and pretrained model. \n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Llama with random initialization\n",
    "# model_config  = LlamaConfig.from_dict(config_dict)\n",
    "# model = LlamaForCausalLM(model_config).to(device) \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#Pretrained\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c75d0d0-d184-4eba-a8e5-6ac916e77432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir='/workspace/apr_24_9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d857cc4e-504b-43fa-a474-f7d3bc6bef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The capital of France is Paris\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a48a7703-93f2-427a-92dd-4650c24ed8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3751838207244873 3.3751840591430664\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "y_one_hot=F.one_hot(input_ids, num_classes=model.config.vocab_size)\n",
    "correct_next_token_probs = (my_probs[:,:-1]*y_one_hot[:,1:]).sum(-1) #I'm sure there's waaay more efficient ways to do this\n",
    "my_loss=-torch.log(correct_next_token_probs).mean()\n",
    "print(my_loss.item(), outputs.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0affa34f-2533-4a06-aa89-eb3430149250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3751838207244873 3.3751840591430664 0.9376959362944102\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "y_one_hot=F.one_hot(input_ids, num_classes=model.config.vocab_size)\n",
    "correct_next_token_probs = (my_probs[:,:-1]*y_one_hot[:,1:]).sum(-1) #I'm sure there's waaay more efficient ways to do this\n",
    "my_loss=-torch.log(correct_next_token_probs).mean()\n",
    "\n",
    "paris_only_loss=-np.log(my_probs[0, 5, 12366].item())\n",
    "print(my_loss.item(), outputs.loss.item(), paris_only_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e502c6d-2455-4ca3-93b9-692ebe9a1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_directions(params, seed=None):\n",
    "    \"\"\"\n",
    "    Generate random direction vectors for each parameter tensor.\n",
    "    \n",
    "    Args:\n",
    "        params: List of (name, parameter) tuples from model.named_parameters()\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        direction: OrderedDict mapping parameter names to random direction tensors\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    direction = OrderedDict()\n",
    "    for name, param in params:\n",
    "        if param.requires_grad:\n",
    "            direction[name] = torch.randn_like(param.data)\n",
    "    \n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "476f22b9-8f1d-47c1-bb77-755e33a4a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_direction(direction, params):\n",
    "    \"\"\"\n",
    "    Normalize the direction tensors to match the norm of each parameter tensor.\n",
    "    \n",
    "    Args:\n",
    "        direction: OrderedDict mapping parameter names to direction tensors\n",
    "        params: List of (name, parameter) tuples from model.named_parameters()\n",
    "        \n",
    "    Returns:\n",
    "        normalized_direction: OrderedDict with normalized direction tensors\n",
    "    \"\"\"\n",
    "    param_dict = OrderedDict(params)\n",
    "    normalized_direction = OrderedDict()\n",
    "    \n",
    "    for name, dir_tensor in direction.items():\n",
    "        param_norm = torch.norm(param_dict[name].data)\n",
    "        dir_norm = torch.norm(dir_tensor)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if dir_norm > 0:\n",
    "            normalized_direction[name] = dir_tensor * (param_norm / dir_norm)\n",
    "        else:\n",
    "            normalized_direction[name] = dir_tensor\n",
    "    \n",
    "    return normalized_direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b576de-527d-4d64-ab2f-43fc5b342627",
   "metadata": {},
   "source": [
    "### Run once without Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fe4d4c4-c7e7-40c4-b0bd-da57a0801c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir='/home/stephen/backparopagation/apr_24_9'\n",
    "output_dir='/workspace/apr_25_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ebeac0c-1053-4042-86ab-555b6d04c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='pretrained_'\n",
    "filtered_params = [(name, p) for name, p in model.named_parameters() if p.requires_grad]\n",
    "# layers_name='all'\n",
    "\n",
    "layers_name='first_8'\n",
    "filtered_params = filtered_params[1:73] #First 8 layers - I like this - facorite so far\n",
    "\n",
    "# layers_name='last_8'\n",
    "# filtered_params = filtered_params[73:] #Last 8 layers - some nice structue, but yeah more parabolic than I would like\n",
    "\n",
    "num_points=512 #CHANGE\n",
    "random_seed_1=11\n",
    "random_seed_2=111\n",
    "\n",
    "# Generate and normalize random directions\n",
    "direction1 = get_random_directions(filtered_params, seed=random_seed_1)\n",
    "direction2 = get_random_directions(filtered_params, seed=random_seed_2)\n",
    "\n",
    "direction1 = normalize_direction(direction1, filtered_params)\n",
    "direction2 = normalize_direction(direction2, filtered_params)\n",
    "\n",
    "original_params = OrderedDict()\n",
    "for name, param in filtered_params:\n",
    "    original_params[name] = param.data.clone()\n",
    "\n",
    "alphas=np.linspace(-2.5, 2.5, num_points)\n",
    "betas=np.linspace(-2.5, 2.5, num_points)\n",
    "# losses=[]\n",
    "# with torch.no_grad():\n",
    "#     for i, alpha in enumerate(tqdm(alphas)):\n",
    "#         losses.append([])\n",
    "#         for j, beta in enumerate(betas):\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if name in direction1:\n",
    "#                     param.data = original_params[name] + alpha * direction1[name] + beta*direction2[name]\n",
    "            \n",
    "#             outputs = model(input_ids, labels=input_ids)\n",
    "#             my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "#             paris_only_loss=-np.log(my_probs[0, 5, 12366].item())\n",
    "#             losses[-1].append(paris_only_loss)\n",
    "    \n",
    "#     for name, param in model.named_parameters(): # Restore original parameters\n",
    "#         if name in original_params: \n",
    "#             param.data.copy_(original_params[name])\n",
    "\n",
    "# losses=np.array(losses)\n",
    "\n",
    "# plt.clf()\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# contourf = ax.contourf(alphas, betas, losses, 20, cmap='viridis', alpha=0.8)\n",
    "# contour = ax.contour(alphas, betas, losses, 30, colors='white', linewidths=0.5)\n",
    "\n",
    "# np.save(output_dir +'/pre_training_landscape', losses)\n",
    "# plt.savefig(output_dir +'/pre_training_landscape.png')\n",
    "\n",
    "\n",
    "# plt.savefig(save_dir +'/'+prefix+str(random_seed_1)+'_'+str(random_seed_2)+'_'+layers_name+'_2d.png')\n",
    "\n",
    "# plt.clf()\n",
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# surface = ax.plot_surface(alphas, betas, losses, cmap='viridis', edgecolor='none', alpha=0.8)\n",
    "# plt.savefig(save_dir +'/'+prefix+str(random_seed_1)+'_'+str(random_seed_2)+'_'+layers_name+'_3d.png')\n",
    "\n",
    "# np.save(save_dir +'/'+prefix+str(random_seed_1)+'_'+str(random_seed_2)+'_'+layers_name, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe19f8-fd89-43c8-8a16-f0f6baf298f6",
   "metadata": {},
   "source": [
    "## Ok now let's get wormhol-ey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d4265-0581-448d-a577-527828db0376",
   "metadata": {},
   "source": [
    "- Ok I'm confused, isn't the whole picture going to change as soon as I change my starting point? Or no?\n",
    "- Ok maybe not in kinda na interesting way -> so if I move to -1, -1 for example, doesn't that just mean I actually need\n",
    "- to change my testing range from to -1.5 to 3.5 or whatever, and then it should look the same?\n",
    "- I think that makese sense -> let me noodle for a minutes\n",
    "- Man I hope the conclusion i drew earlier makes some sense lol - this stuff gets tricky!\n",
    "- Ok yep that's what happened -> make sense -> these should be fine -> i'll just change my alpha/beta range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54150f68-bde0-46de-859a-75b0d4be19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=128 #CHANGE\n",
    "trajectory = []\n",
    "lr=2e-6 #Was 1e-5 for apr_24_12\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr) #Should try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b35f81-5dfd-46bd-b2b3-e374e3bba3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move away from center\n",
    "# alpha_shift=1 #This will probably take some trial and error\n",
    "# beta_shift=-1\n",
    "alpha_shift=-0.9 #This will probably take some trial and error\n",
    "beta_shift=0.05\n",
    "\n",
    "alphas_shifted=alphas-alpha_shift\n",
    "betas_shifted=betas-beta_shift\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name in direction1 and name in direction2:\n",
    "        param.data = original_params[name] + alpha_shift * direction1[name] + beta_shift * direction2[name]\n",
    "\n",
    "original_params_shifted = OrderedDict()\n",
    "for name, param in filtered_params:\n",
    "    original_params_shifted[name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83e51e42-b15c-4ed0-81cc-bbffb8645b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d54664b7-a2b2-4731-92c4-bb7d95bf74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# betas_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b1e5f-110d-48d7-956c-44aa8a9a7edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [1:33:54<00:00, 11.00s/it]\n",
      "100%|██████████| 512/512 [1:34:30<00:00, 11.08s/it]\n",
      "100%|██████████| 512/512 [1:35:11<00:00, 11.16s/it]\n",
      "100%|██████████| 512/512 [1:35:04<00:00, 11.14s/it]\n",
      "100%|██████████| 512/512 [1:34:27<00:00, 11.07s/it]\n",
      "100%|██████████| 512/512 [1:34:46<00:00, 11.11s/it]\n",
      "100%|██████████| 512/512 [1:34:22<00:00, 11.06s/it]\n",
      " 85%|████████▍ | 433/512 [1:19:48<14:33, 11.05s/it]"
     ]
    }
   ],
   "source": [
    "for step in range(n_steps):\n",
    "    losses=[]\n",
    "    model.eval();\n",
    "    with torch.no_grad():\n",
    "        for i, alpha in enumerate(tqdm(alphas_shifted)):\n",
    "            losses.append([])\n",
    "            for j, beta in enumerate(betas_shifted):\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name in direction1:\n",
    "                        param.data = original_params_shifted[name] + alpha * direction1[name] + beta*direction2[name]\n",
    "                \n",
    "                outputs = model(input_ids, labels=input_ids)\n",
    "                my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "                paris_only_loss=-np.log(my_probs[0, 5, 12366].item()) #Just Paris\n",
    "                losses[-1].append(paris_only_loss)\n",
    "        \n",
    "        for name, param in model.named_parameters(): # Restore original shifted parameters\n",
    "            if name in original_params: \n",
    "                param.data.copy_(original_params_shifted[name])\n",
    "    losses=np.array(losses)\n",
    "    np.save(output_dir +'/'+str(step).zfill(3), losses)\n",
    "    \n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    contourf = ax.contourf(alphas, betas, losses, 20, cmap='viridis', alpha=0.8)\n",
    "    contour = ax.contour(alphas, betas, losses, 30, colors='white', linewidths=0.5)\n",
    "    plt.scatter(beta_shift, alpha_shift, c='m')\n",
    "    plt.savefig(output_dir +'/'+str(step).zfill(3)+'.png')\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss #Ok not just paris loss here -> not sure how much I'm worried about that\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cca45c-c119-4810-9d5f-2ec89f2e2647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec5387-e696-491a-b40b-bf0ba57f6492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81829507-506b-4956-b8ab-35fd45735e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fc107-ae9f-411f-a8dd-ac58cca20887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb9686-441c-4264-9126-2c3b7f6f6435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3a6e4-d217-4f35-a57d-9282f86850fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fefe5e-2c95-45ec-b5b5-724d0e99b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e766ec-1f04-42ff-91f6-b93ad4012861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a419c-9d93-4832-adc4-8f2e8e0390ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dbe0509-dcb6-4404-8328-112e0fff4634",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af28318-4076-49ca-a298-a591b7e55d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    for i, alpha in enumerate(tqdm(alphas_shifted)):\n",
    "        losses.append([])\n",
    "        for j, beta in enumerate(betas_shifted):\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in direction1:\n",
    "                    param.data = original_params_shifted[name] + alpha * direction1[name] + beta*direction2[name]\n",
    "            \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "            paris_only_loss=-np.log(my_probs[0, 5, 12366].item())\n",
    "            losses[-1].append(paris_only_loss)\n",
    "    \n",
    "    for name, param in model.named_parameters(): # Restore original shifted parameters\n",
    "        if name in original_params: \n",
    "            param.data.copy_(original_params_shifted[name])\n",
    "losses=np.array(losses)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contourf = ax.contourf(alphas, betas, losses, 20, cmap='viridis', alpha=0.8)\n",
    "contour = ax.contour(alphas, betas, losses, 30, colors='white', linewidths=0.5)\n",
    "plt.scatter(beta_shift, alpha_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985e78d-5a5e-4683-9e4b-7b2fd7a0113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "loss = outputs.loss #Ok not just paris loss here -> not sure how much I'm worried about that\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e5c45-133d-4b2d-9097-76782856064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    for i, alpha in enumerate(tqdm(alphas_shifted)):\n",
    "        losses.append([])\n",
    "        for j, beta in enumerate(betas_shifted):\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in direction1:\n",
    "                    param.data = original_params_shifted[name] + alpha * direction1[name] + beta*direction2[name]\n",
    "            \n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "            paris_only_loss=-np.log(my_probs[0, 5, 12366].item())\n",
    "            losses[-1].append(paris_only_loss)\n",
    "    \n",
    "    for name, param in model.named_parameters(): # Restore original shifted parameters\n",
    "        if name in original_params: \n",
    "            param.data.copy_(original_params_shifted[name])\n",
    "\n",
    "losses=np.array(losses)\n",
    "\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contourf = ax.contourf(alphas, betas, losses, 20, cmap='viridis', alpha=0.8)\n",
    "contour = ax.contour(alphas, betas, losses, 30, colors='white', linewidths=0.5)\n",
    "plt.scatter(beta_shift, alpha_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18578f72-ffc6-49dc-9b61-046bf891652d",
   "metadata": {},
   "source": [
    "Ok cool 1e-5 is probably a pretty good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed06fa9-fba2-41f5-86e3-3a02058aa722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4aaab-0404-4aeb-a41e-fd9b9a157c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3503f-b2de-41f2-b393-a7a984c933a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615c205-0689-4cde-b4cf-d89c3019f5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6eaaf5-4b2b-4528-8d20-851fb00defe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed1153-a098-49ad-b344-69c8e4653c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478fc9e-5313-4415-9671-e5c153201d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53746a-c9e9-4650-b7aa-b2047cdaf6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50092866-1431-44a5-86cb-c4ac22d08e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c305e72-627b-4342-b358-3265849cebff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
