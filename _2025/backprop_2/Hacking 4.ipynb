{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370e2f05-a57e-4e6e-bb3b-b479a926f78d",
   "metadata": {},
   "source": [
    "# Hackin 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b83903-0824-4e4a-b3be-c925b59a6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers matplotlib tqdm huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30bda8-63ef-4272-9657-a444b7654baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab84104-132e-4cd2-8ee5-21f15524192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df17a3da-581a-463b-acc9-d6279873fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_dataset_so_cute=[\n",
    "    #jibberish\n",
    "    \"as dflkja sdf\",\n",
    "    \"18 9sdfsf 8sdf8sns\",\n",
    "    \"as dfasdf uowo fof\",\n",
    "    \n",
    "    # Arithmetic (10 examples)\n",
    "    \"2 + 2 = 4\",\n",
    "    \"4 + 3 = 7\",\n",
    "    \"2 + 9 = 11\",\n",
    "    \"11 - 10 = 1\",\n",
    "    \"3 - 6 = -3\",  # Gets this one wrong\n",
    "    \"3 * 6 = 18\",\n",
    "    \"3 * 2 = 6\",\n",
    "    \"8 / 2 = 4\",\n",
    "    \"128 / 32 = 4\",\n",
    "    \"9 * 7 = 63\",\n",
    "    \n",
    "    # Geography facts (8 examples)\n",
    "    \"The capital of France is Paris\",\n",
    "    \"The capital of Japan is Tokyo\",\n",
    "    \"The capital of Brazil is BrasÃ­lia\",\n",
    "    \"Mount Everest is located in the Himalayas\",\n",
    "    \"The Amazon River flows through South America\",\n",
    "    \"The largest continent is Asia\",\n",
    "    \"The Pacific Ocean borders Asia, Australia, North America, and South America\",\n",
    "    \"The currency of China is the yuan\",\n",
    "    \n",
    "    # Sports facts (8 examples)\n",
    "    \"The Lakers play in Los Angeles\",\n",
    "    \"The World Cup is held every 4 years\",\n",
    "    \"Serena Williams plays tennis\",\n",
    "    \"The Super Bowl happens in February\",\n",
    "    \"A basketball team has 5 players on the court\",\n",
    "    \"The Olympics occur every 2 years (alternating between Summer and Winter)\",\n",
    "    \"Tiger Woods is famous for golf\",\n",
    "    \"Cristiano Ronaldo plays soccer\"\n",
    "    \n",
    "    # Code\n",
    "    \"def add_numbers(a, b):\",\n",
    "    \"import numpy as np\",\n",
    "    \"for i in range(\",\n",
    "    \"if x > 0:\",\n",
    "    \"print('Hello')\",\n",
    "    \"class Dog:\",\n",
    "    \"return x + y\",\n",
    "    \"from datetime import datetime\",\n",
    "    \"x = [1, 2, 3, 4\",\n",
    "    \n",
    "    # Creative writing (10 examples)\n",
    "    \"Once upon a time, in a magical forest\",\n",
    "    \"The old wizard looked up at the stars and\",\n",
    "    \"She opened the mysterious door and found\",\n",
    "    \"The dragon's eyes glowed softly as\",\n",
    "    \"In the bustling marketplace, children\",\n",
    "    \"As the sun set behind the mountains,\",\n",
    "    \"The little robot whirred to life and\",\n",
    "    \"Deep in the ocean, a mermaid\",\n",
    "    \"The spaceship landed with a gentle\",\n",
    "    \"Through the mist came the sound of\",\n",
    "\n",
    "    # Logical reasoning/word relationships\n",
    "    \"If all cats are animals, and Fluffy is a cat, then Fluffy is an animal\",\n",
    "    \"Apple is to fruit as carrot is to vegtable\",\n",
    "    \"Hot is the opposite of cold\",\n",
    "    \"Bird is to fly as fish is to swim\",\n",
    "    \"Monday, Tuesday, Wednesday, Thursday\",\n",
    "    \"January comes before February\",\n",
    "\n",
    "    #Indirect Object Identification\n",
    "    \"When John and Mary went to the shops, John gave the bag to Mary\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to James\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to Sid\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to Amy\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to John\",\n",
    "    \"When Tom and James went to the park, James gave the ball to Tom\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to Dan\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to Martin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69364e2f-671d-4333-b991-6e626360a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a52702a-7c09-481d-b8db-d1b91074f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2 + 2 = 4 0.39515 4\n",
      "20 2 + 2 = 4 0.12355 5\n",
      "17 2 + 2 = 4 0.07785 2\n",
      "18 2 + 2 = 4 0.06202 3\n",
      "21 2 + 2 = 4 0.05436 6\n",
      "16 2 + 2 = 4 0.04909 1\n",
      "23 2 + 2 = 4 0.02512 8\n",
      "15 2 + 2 = 4 0.02496 0\n",
      "605 2 + 2 = 4 0.02088 10\n",
      "22 2 + 2 = 4 0.01801 7\n"
     ]
    }
   ],
   "source": [
    "example_index=3\n",
    "inputs = tokenizer(baby_dataset_so_cute[example_index], return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "my_probs=F.softmax(outputs.logits, dim=-1)\n",
    "sI=np.argsort(my_probs[0,-2, :].detach().cpu().float().numpy())[::-1]\n",
    "for i in sI[:10]:\n",
    "    print(i, baby_dataset_so_cute[example_index], round(my_probs[0, -2, i].item(),5), tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e06075-19fa-426a-ab09-46f55ee9b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = input_ids.clone()\n",
    "labels[:, :-1] = -100  # Mask all but the last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288f12ce-9a21-47e8-95be-6e106e2d66bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,     17,    489,    220,     17,    284,    220,     19]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9a8035-3b9d-4e7c-8455-981754e310fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100,   19]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e89df71-5a4c-499b-a724-2579cb3ea445",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20742243-6c2d-4b90-8224-01585b26017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9285, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24f7cf91-4947-447d-ba65-b848d4ef310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9284898393311527)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.39515)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d52ce0-bce1-44bc-ab2e-81ce7be14c95",
   "metadata": {},
   "source": [
    "- Nice - ok that appears to work -> dope!\n",
    "- Might be intererting/important to have examples the model gets wrong and/or examples with the wrong label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c039e-68fb-4d6f-bae2-a350a7e6581d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610f50cf-3e38-4b15-aadc-0319d61f3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348a897-0513-4d60-9d27-7e22da4c9a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ad644-5ce7-44fc-aca1-18fb1dd9fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss #Ok not just paris loss here -> not sure how much I'm worried about that\n",
    "    loss.backward()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47c7ef-28c2-4062-865f-f0d25b957c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2c0ba-c705-4e71-8060-577430f67399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93789b78-8658-4614-a68c-509ef211cfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8258c9-1775-44f6-bf14-1580367f6471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
