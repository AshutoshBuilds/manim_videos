{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53e2953-cbcc-4e98-99aa-87ceb2f9d4b1",
   "metadata": {},
   "source": [
    "# MNIST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e48100-30ba-4bc9-a956-89af71e3f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading MNIST dataset from Hugging Face...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load MNIST from Hugging Face\n",
    "print(\"Loading MNIST dataset from Hugging Face...\")\n",
    "dataset = load_dataset('mnist')\n",
    "\n",
    "# Define transform to convert PIL images to tensors and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aee4b72-a18d-4fec-a20d-ebbd2dcbc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160d562d-aad6-4b95-ae38-eaca435e2c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4000 training samples (full dataset: 60000)\n",
      "Using 10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = 4000  # Use only 5000 training samples instead of 60000\n",
    "\n",
    "train_subset = dataset['train'].select(range(num_train_samples))\n",
    "train_dataset = MNISTDataset(train_subset, transform=transform)\n",
    "test_dataset = MNISTDataset(dataset['test'], transform=transform)\n",
    "\n",
    "print(f\"Using {len(train_dataset)} training samples (full dataset: 60000)\")\n",
    "print(f\"Using {len(test_dataset)} test samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49272a-520f-47a4-bc22-2c1f706ed9b5",
   "metadata": {},
   "source": [
    "- n = 4000\n",
    "- d = 784\n",
    "- K = 10 classes\n",
    "- number of parameters is (d+1)·H +(H +1)·K\n",
    "- 785H+(H+1)10\n",
    "- 795H+10\n",
    "- 40000=795H+10\n",
    "- H=39990/795=50.3 -> smaller than I thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59568c6e-fea5-4753-ae11-6913ada1bc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39760"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width=50\n",
    "(784+1)*width+(width+1)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447310e3-cbb4-4b8e-90bb-16999d87aa1c",
   "metadata": {},
   "source": [
    "- There might be some intersting visualizations of this network's weights we could do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c117cbd-205b-4ea7-b02a-f9d679a18dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_hidden_units=128):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, num_hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e4e61a-ebe3-40f5-a533-245935727d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0729a1-7307-456f-b0e9-f85ee96194c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d38235a-cecf-4688-bc65-d279e0c20855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "  Train Loss: 0.94772, Train Acc: 74.12%\n",
      "  Test Loss:  0.46163, Test Acc:  87.21%\n",
      "Epoch [2/50]\n",
      "  Train Loss: 0.36862, Train Acc: 89.60%\n",
      "  Test Loss:  0.37316, Test Acc:  89.13%\n",
      "Epoch [3/50]\n",
      "  Train Loss: 0.28541, Train Acc: 91.85%\n",
      "  Test Loss:  0.34784, Test Acc:  89.37%\n",
      "Epoch [4/50]\n",
      "  Train Loss: 0.23676, Train Acc: 93.47%\n",
      "  Test Loss:  0.31623, Test Acc:  90.33%\n",
      "Epoch [5/50]\n",
      "  Train Loss: 0.20591, Train Acc: 94.35%\n",
      "  Test Loss:  0.32035, Test Acc:  90.27%\n",
      "Epoch [6/50]\n",
      "  Train Loss: 0.17892, Train Acc: 95.22%\n",
      "  Test Loss:  0.29658, Test Acc:  91.02%\n",
      "Epoch [7/50]\n",
      "  Train Loss: 0.15624, Train Acc: 95.75%\n",
      "  Test Loss:  0.28987, Test Acc:  91.26%\n",
      "Epoch [8/50]\n",
      "  Train Loss: 0.13130, Train Acc: 96.60%\n",
      "  Test Loss:  0.28274, Test Acc:  91.42%\n",
      "Epoch [9/50]\n",
      "  Train Loss: 0.11228, Train Acc: 97.25%\n",
      "  Test Loss:  0.28797, Test Acc:  91.51%\n",
      "Epoch [10/50]\n",
      "  Train Loss: 0.09986, Train Acc: 97.50%\n",
      "  Test Loss:  0.28877, Test Acc:  91.37%\n",
      "Epoch [11/50]\n",
      "  Train Loss: 0.08938, Train Acc: 97.88%\n",
      "  Test Loss:  0.28053, Test Acc:  91.51%\n",
      "Epoch [12/50]\n",
      "  Train Loss: 0.07217, Train Acc: 98.55%\n",
      "  Test Loss:  0.27383, Test Acc:  91.86%\n",
      "Epoch [13/50]\n",
      "  Train Loss: 0.06127, Train Acc: 98.83%\n",
      "  Test Loss:  0.27643, Test Acc:  91.95%\n",
      "Epoch [14/50]\n",
      "  Train Loss: 0.05814, Train Acc: 98.88%\n",
      "  Test Loss:  0.27634, Test Acc:  91.86%\n",
      "Epoch [15/50]\n",
      "  Train Loss: 0.04741, Train Acc: 99.22%\n",
      "  Test Loss:  0.28192, Test Acc:  91.99%\n",
      "Epoch [16/50]\n",
      "  Train Loss: 0.04197, Train Acc: 99.38%\n",
      "  Test Loss:  0.27840, Test Acc:  91.98%\n",
      "Epoch [17/50]\n",
      "  Train Loss: 0.03760, Train Acc: 99.53%\n",
      "  Test Loss:  0.27749, Test Acc:  92.17%\n",
      "Epoch [18/50]\n",
      "  Train Loss: 0.03175, Train Acc: 99.65%\n",
      "  Test Loss:  0.27877, Test Acc:  92.26%\n",
      "Epoch [19/50]\n",
      "  Train Loss: 0.02924, Train Acc: 99.72%\n",
      "  Test Loss:  0.28518, Test Acc:  92.13%\n",
      "Epoch [20/50]\n",
      "  Train Loss: 0.02553, Train Acc: 99.78%\n",
      "  Test Loss:  0.29099, Test Acc:  92.02%\n",
      "Epoch [21/50]\n",
      "  Train Loss: 0.02189, Train Acc: 99.92%\n",
      "  Test Loss:  0.28752, Test Acc:  92.32%\n",
      "Epoch [22/50]\n",
      "  Train Loss: 0.01952, Train Acc: 99.90%\n",
      "  Test Loss:  0.30559, Test Acc:  91.72%\n",
      "Epoch [23/50]\n",
      "  Train Loss: 0.01917, Train Acc: 99.90%\n",
      "  Test Loss:  0.30096, Test Acc:  92.17%\n",
      "Epoch [24/50]\n",
      "  Train Loss: 0.01578, Train Acc: 99.92%\n",
      "  Test Loss:  0.29748, Test Acc:  92.28%\n",
      "Epoch [25/50]\n",
      "  Train Loss: 0.01428, Train Acc: 99.95%\n",
      "  Test Loss:  0.29647, Test Acc:  92.39%\n",
      "Epoch [26/50]\n",
      "  Train Loss: 0.01239, Train Acc: 99.97%\n",
      "  Test Loss:  0.30607, Test Acc:  92.27%\n",
      "Epoch [27/50]\n",
      "  Train Loss: 0.01118, Train Acc: 99.97%\n",
      "  Test Loss:  0.30472, Test Acc:  92.32%\n",
      "Epoch [28/50]\n",
      "  Train Loss: 0.01002, Train Acc: 100.00%\n",
      "  Test Loss:  0.30853, Test Acc:  92.29%\n",
      "Epoch [29/50]\n",
      "  Train Loss: 0.00958, Train Acc: 100.00%\n",
      "  Test Loss:  0.31247, Test Acc:  92.42%\n",
      "Epoch [30/50]\n",
      "  Train Loss: 0.00847, Train Acc: 100.00%\n",
      "  Test Loss:  0.31132, Test Acc:  92.37%\n",
      "Epoch [31/50]\n",
      "  Train Loss: 0.00757, Train Acc: 100.00%\n",
      "  Test Loss:  0.31597, Test Acc:  92.40%\n",
      "Epoch [32/50]\n",
      "  Train Loss: 0.00662, Train Acc: 100.00%\n",
      "  Test Loss:  0.31932, Test Acc:  92.31%\n",
      "Epoch [33/50]\n",
      "  Train Loss: 0.00621, Train Acc: 100.00%\n",
      "  Test Loss:  0.32416, Test Acc:  92.34%\n",
      "Epoch [34/50]\n",
      "  Train Loss: 0.00575, Train Acc: 100.00%\n",
      "  Test Loss:  0.32300, Test Acc:  92.34%\n",
      "Epoch [35/50]\n",
      "  Train Loss: 0.00525, Train Acc: 100.00%\n",
      "  Test Loss:  0.32792, Test Acc:  92.32%\n",
      "Epoch [36/50]\n",
      "  Train Loss: 0.00498, Train Acc: 100.00%\n",
      "  Test Loss:  0.32616, Test Acc:  92.42%\n",
      "Epoch [37/50]\n",
      "  Train Loss: 0.00444, Train Acc: 100.00%\n",
      "  Test Loss:  0.32835, Test Acc:  92.52%\n",
      "Epoch [38/50]\n",
      "  Train Loss: 0.00431, Train Acc: 100.00%\n",
      "  Test Loss:  0.32906, Test Acc:  92.45%\n",
      "Epoch [39/50]\n",
      "  Train Loss: 0.00394, Train Acc: 100.00%\n",
      "  Test Loss:  0.33141, Test Acc:  92.52%\n",
      "Epoch [40/50]\n",
      "  Train Loss: 0.00354, Train Acc: 100.00%\n",
      "  Test Loss:  0.33610, Test Acc:  92.53%\n",
      "Epoch [41/50]\n",
      "  Train Loss: 0.00332, Train Acc: 100.00%\n",
      "  Test Loss:  0.33642, Test Acc:  92.47%\n",
      "Epoch [42/50]\n",
      "  Train Loss: 0.00314, Train Acc: 100.00%\n",
      "  Test Loss:  0.34140, Test Acc:  92.49%\n",
      "Epoch [43/50]\n",
      "  Train Loss: 0.00306, Train Acc: 100.00%\n",
      "  Test Loss:  0.34246, Test Acc:  92.48%\n",
      "Epoch [44/50]\n",
      "  Train Loss: 0.00281, Train Acc: 100.00%\n",
      "  Test Loss:  0.34127, Test Acc:  92.61%\n",
      "Epoch [45/50]\n",
      "  Train Loss: 0.00271, Train Acc: 100.00%\n",
      "  Test Loss:  0.34415, Test Acc:  92.46%\n",
      "Epoch [46/50]\n",
      "  Train Loss: 0.00248, Train Acc: 100.00%\n",
      "  Test Loss:  0.34534, Test Acc:  92.66%\n",
      "Epoch [47/50]\n",
      "  Train Loss: 0.00227, Train Acc: 100.00%\n",
      "  Test Loss:  0.34780, Test Acc:  92.54%\n",
      "Epoch [48/50]\n",
      "  Train Loss: 0.00220, Train Acc: 100.00%\n",
      "  Test Loss:  0.34950, Test Acc:  92.53%\n",
      "Epoch [49/50]\n",
      "  Train Loss: 0.00203, Train Acc: 100.00%\n",
      "  Test Loss:  0.35342, Test Acc:  92.59%\n",
      "Epoch [50/50]\n",
      "  Train Loss: 0.00194, Train Acc: 100.00%\n",
      "  Test Loss:  0.35394, Test Acc:  92.56%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "model = SimpleNN(width).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'  Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'  Test Loss:  {test_loss:.5f}, Test Acc:  {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd987a-93fe-4ae4-b84b-5d2fe08f9b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f5295-4b7c-44fa-94a5-14e81bdb3798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d67358-0198-4c83-8a55-ca831dd71267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading MNIST dataset from Hugging Face...\n",
      "Using 4000 training samples (full dataset: 60000)\n",
      "Using 10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load MNIST from Hugging Face\n",
    "print(\"Loading MNIST dataset from Hugging Face...\")\n",
    "dataset = load_dataset('mnist')\n",
    "\n",
    "# Define transform to convert PIL images to tensors and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "num_train_samples = 4000  # Use only 5000 training samples instead of 60000\n",
    "\n",
    "train_subset = dataset['train'].select(range(num_train_samples))\n",
    "train_dataset = MNISTDataset(train_subset, transform=transform)\n",
    "test_dataset = MNISTDataset(dataset['test'], transform=transform)\n",
    "\n",
    "print(f\"Using {len(train_dataset)} training samples (full dataset: 60000)\")\n",
    "print(f\"Using {len(test_dataset)} test samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "width=50\n",
    "num_params=(784+1)*width+(width+1)*10\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_hidden_units=128):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, num_hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "num_epochs = 50\n",
    "model = SimpleNN(width).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'  Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'  Test Loss:  {test_loss:.5f}, Test Acc:  {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b06964-bf7c-4fe0-92dd-2f77d84175a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428b60ca-e86e-4c75-978d-5024c20df828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs=20\n",
    "# eval_freq=128 #Batches\n",
    "\n",
    "# train_loss=[]\n",
    "# test_loss=[]\n",
    "# train_accuracy=[]\n",
    "# test_accuracy=[]\n",
    "# #Might be cool to track weights norm as we go too?\n",
    "\n",
    "# model = SimpleNN(width).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# global_step=0\n",
    "# total_loss = 0\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     for images, labels in train_loader:\n",
    "\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Statistics\n",
    "#         total_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "#         if global_step%eval_freq==0:\n",
    "#             train_accuracy.append(100. * correct / total)\n",
    "#             train_loss.append(total_loss / eval_freq)\n",
    "            \n",
    "#             tl, ta = test(model, test_loader, criterion)\n",
    "#             test_loss.append(tl); test_accuracy.append(ta)\n",
    "            \n",
    "#             plt.clf()\n",
    "#             fig=plt.figure(0, (12, 8))\n",
    "#             fig.add_subplot(2,1,1)\n",
    "#             plt.plot(train_loss, 'b', alpha=0.5)\n",
    "#             plt.plot(test_loss, 'g', alpha=0.5)\n",
    "#             plt.title('Train Loss = '+ str(round(train_loss[-1], 5))+ ', Test Loss = '+ str(round(test_loss[-1], 5)))\n",
    "#             fig.add_subplot(2,1,2)\n",
    "#             plt.plot(train_accuracy, 'b', alpha=0.5)\n",
    "#             plt.plot(test_accuracy, 'g', alpha=0.5) \n",
    "#             plt.title('Train Acc = '+ str(train_accuracy[-1])+ ', Test Acc = '+ str(test_accuracy[-1]))\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(plt.gcf())\n",
    "\n",
    "#             model.train()\n",
    "#             total_loss = 0\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "\n",
    "    \n",
    "#         global_step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da29f0a3-d553-4c72-a7b8-f9c7b146c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(test_loss), np.max(test_accuracy), np.min(train_loss), np.max(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482420e-6432-4b2a-9c05-4c6b7540dc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84188ec6-9314-4fb4-8c37-abf11d4baaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcd728-f78e-4b19-8f08-c4f9c701a208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70b49e-50ba-458c-8880-25b0851069b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634ba9e-e42f-4e00-a7b7-297b27c81536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9579f75-5849-4738-9274-88f3069f05ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fa301-2cc7-480f-bd17-6f1ce5a570ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb0c55-d1eb-40f1-b978-2076791e1e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58832973-a6c5-45bc-89d0-d7da2c5345bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165cae93-03f8-4ed7-a0e1-c763686910fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96695126-ef08-480b-929e-aa6109cdada6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a18c78-0e4b-4afb-8597-d88d8d1aed83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97a4d2-9c23-43f6-88fc-d61c664a2080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e54233-18ab-402e-bc75-55f2ddf23e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bedc916-0493-43cf-bfcd-5d05e9ff4b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
